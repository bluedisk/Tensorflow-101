{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Hangul RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import string\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import hgtk\n",
    "\n",
    "from six.moves import cPickle\n",
    "from TextLoader import *\n",
    "\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset using TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "type of 'data_loader' is <class 'dict'>, length is 76\n",
      "\n",
      "\n",
      "data_loader.vocab looks like \n",
      "{'\"': 28, 'ㅢ': 25, 'ㅛ': 38, '1': 58, '9': 64, '\\n': 19, 'ㄷ': 13, 'ㄾ': 70, 'ㅈ': 15, '(': 45, 'ㅄ': 43, '8': 68, 'ㅏ': 3, '>': 75, '5': 63, 'ㅠ': 37, ')': 46, 'ㅜ': 16, '6': 59, '\\x1a': 72, 'ㅀ': 51, 'ㅗ': 9, 'ㅌ': 30, '3': 66, '0': 73, 'ㅃ': 47, 'ㅎ': 14, 'ㅁ': 12, 'ㅣ': 6, 'ㅝ': 36, 'ㄴ': 4, ' ': 1, 'ㄻ': 55, 'ㅖ': 39, 'ㅓ': 11, '.': 24, 'ㅚ': 33, 'ㅉ': 40, 'ㅘ': 26, 'ㄱ': 7, 'ㅂ': 17, 'ㅆ': 22, '!': 52, ':': 57, 'ㄵ': 50, 'ㄺ': 48, 'ㄳ': 60, 'ㅔ': 20, 'ㅊ': 23, '4': 67, 'ㄿ': 56, ',': 27, 'ㅑ': 34, 'ㄶ': 44, '7': 71, 'ㅋ': 42, 'ㄸ': 32, '-': 65, 'ㅍ': 31, 'ㄲ': 29, 'ㅙ': 53, 'ㅞ': 61, '2': 62, 'ㄼ': 54, 'ㅕ': 18, 'ㅒ': 74, 'ㅡ': 8, '_': 69, '?': 41, 'ㅇ': 2, 'ᴥ': 0, 'ㅐ': 21, \"'\": 49, 'ㅟ': 35, 'ㅅ': 10, 'ㄹ': 5} \n",
      "\n",
      "\n",
      "type of 'data_loader.chars' is <class 'tuple'>, length is 76\n",
      "\n",
      "\n",
      "data_loader.chars looks like \n",
      "('ᴥ', ' ', 'ㅇ', 'ㅏ', 'ㄴ', 'ㄹ', 'ㅣ', 'ㄱ', 'ㅡ', 'ㅗ', 'ㅅ', 'ㅓ', 'ㅁ', 'ㄷ', 'ㅎ', 'ㅈ', 'ㅜ', 'ㅂ', 'ㅕ', '\\n', 'ㅔ', 'ㅐ', 'ㅆ', 'ㅊ', '.', 'ㅢ', 'ㅘ', ',', '\"', 'ㄲ', 'ㅌ', 'ㅍ', 'ㄸ', 'ㅚ', 'ㅑ', 'ㅟ', 'ㅝ', 'ㅠ', 'ㅛ', 'ㅖ', 'ㅉ', '?', 'ㅋ', 'ㅄ', 'ㄶ', '(', ')', 'ㅃ', 'ㄺ', \"'\", 'ㄵ', 'ㅀ', '!', 'ㅙ', 'ㄼ', 'ㄻ', 'ㄿ', ':', '1', '6', 'ㄳ', 'ㅞ', '2', '5', '9', '-', '3', '4', '8', '_', 'ㄾ', '7', '\\x1a', '0', 'ㅒ', '>') \n"
     ]
    }
   ],
   "source": [
    "data_dir    = \"data/nine_dreams\"\n",
    "batch_size  = 50\n",
    "seq_length  = 50\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "# This makes \"vocab.pkl\" and \"data.npy\" in \"data/nine_dreams\"   \n",
    "#  from \"data/nine_dreams/input.txt\" \n",
    "vocab_size = data_loader.vocab_size\n",
    "vocab = data_loader.vocab\n",
    "chars = data_loader.chars\n",
    "print ( \"type of 'data_loader' is %s, length is %d\" \n",
    "       % (type(data_loader.vocab), len(data_loader.vocab)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.vocab looks like \\n%s \" %\n",
    "       (data_loader.vocab))\n",
    "print ( \"\\n\" )\n",
    "print ( \"type of 'data_loader.chars' is %s, length is %d\" \n",
    "       % (type(data_loader.chars), len(data_loader.chars)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.chars looks like \\n%s \" % (data_loader.chars,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "rnn_size   = 512\n",
    "num_layers = 3\n",
    "grad_clip  = 5.\n",
    "\n",
    "_batch_size = 1\n",
    "_seq_length = 1\n",
    "\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Select RNN Cell\n",
    "    unitcell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([unitcell] * num_layers)\n",
    "    # Set paths to the graph \n",
    "    input_data = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    targets    = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    initial_state = cell.zero_state(_batch_size, tf.float32)\n",
    "\n",
    "    # Set Network\n",
    "    with tf.variable_scope('rnnlm'):\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "            inputs = tf.split(tf.nn.embedding_lookup(embedding, input_data), _seq_length, 1)\n",
    "            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "            \n",
    "    # Loop function for seq2seq\n",
    "    def loop(prev, _):\n",
    "        prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "    # Output of RNN \n",
    "    outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state\n",
    "                                , cell, loop_function=None, scope='rnnlm')\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, rnn_size])\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    # Next word probability \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    # Define LOSS\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], # Input\n",
    "        [tf.reshape(targets, [-1])], # Target\n",
    "        [tf.ones([_batch_size * _seq_length])], # Weight \n",
    "        vocab_size)\n",
    "    # Define Optimizer\n",
    "    cost = tf.reduce_sum(loss) / _batch_size / _seq_length\n",
    "    final_state = last_state\n",
    "    lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    _optm = tf.train.AdamOptimizer(lr)\n",
    "    optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling function done.\n"
     ]
    }
   ],
   "source": [
    "# Sample ! \n",
    "def sample( sess, chars, vocab, __probs, num=200, prime=u'ㅇㅗᴥㄴㅡㄹᴥ '):\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    _probs = __probs\n",
    "    prime = list(prime)\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        weights = weights / np.sum(weights) \n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [_probsval, state] = sess.run([_probs, final_state], feed)\n",
    "        p = _probsval[0]\n",
    "        sample = int(np.random.choice(len(p), p=p))\n",
    "        # sample = weighted_pick(p)\n",
    "        # sample = np.argmax(p)\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret\n",
    "print (\"sampling function done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "누\n",
      "구\n",
      " \n",
      "Prime Text : 누구  => ㄴㅜᴥㄱㅜᴥ \n",
      "data/nine_dreams/model.ckpt-0\n",
      "SAMPLED TEXT = ㄴㅜᴥㄱㅜᴥ ?ㅆ>ㅀ79ㄼㅖㅝ5:6ㅜ-8>ㅂ1ㄳㄲㅃ0ㄻㄾ1ㅠㅙㅊ5ㅀ)ㄵ2ㅕ(ㄷ4(ㅊㅎ_ᴥᴥㅌ\u001a>ㅢㅃㅢㅈㅢㅞ 44ㅜㅟ:ㅜㅑ\u001aㅒㄺㅈㅡㄵ72ㅍㅡㅙㅀ0ㅊㅅㅜㅆㅍㅛㄹㄱㅃ>ㄳ_. ㅡㅁㅅㅌ?ㄿㄻㅑ\n",
      "ㅣㄸㅋㅏㅅㅊ2ㅄㅠㄵㅐㅞㄾㅏㄱㄵㄴ?_ㄿㅎㅋ2ㅢ\u001a8ㅚㅘㅎㄶㅀㄲㅅㅍ6ㅔㄶㅋㅍㅈㅕ'7ㅣ72ㅜㅠㅀㅣ>ㄳㅉㅉ,ㅍㅅᴥㄸㅡㅟ5ㅎ5 3ㄳ(4ㅓ4ㅡㄳㅀㅙㄻㅘ24ㅈㅊ!ㅍㄷㅕㄶㅖ>ㅆㄲ6ㅑ50!.ㅠㅀㄿㅊㅍㅇㅜㅈ(ㅜㅠ!ㄻ\"4ㅕ_')ㅖㅒㅂㄲ(ㅐ.ㅖㅍㄳㅢㄷ!\u001a\"ㅓ\u001a?ㅈㅎㄺㅁ,-ㅗㅠㅅㅏ\u001aㅡ.ㅠㄲㅁ.ㅏㄸㅙㅊ 3> ㅠㄸ896ㅡ\n",
      "ㅣㅏ!25ㅀ5ㅏㅋㅞㅃㅚ\u001aㅂㅊㅌ5ㅠㄹㅉ.0ㄼㅔㅟㅕ9ㄳ\"ㅄㄶㅒㄻㅀㅞㅗ3ㅇㅙㅑㅍㅄㄷㅑ.ㄿㄵ6ㅈ,ㅠㅑ?ㅑㄾㅅㅄᴥㅞㅉㅌㅌ'ㅀㅂㅙㄸㄿㅉ0ㅜㄻᴥ>ㅂᴥ!\n",
      ")0ㅅㄱㅛᴥㅋ>ㅉ4ㄾㅃ_ㅁㅈㄶ8ㅄㄵ!ㅓㅎㄱ_ㅔ.ㅆㅇㅍㅘㅅ2ㅏㅌㅁㅡㅁ!ㄹ92ㅣㄻ7)4ㅃ_ㅊ1ㅁㅣㅎㅒㄼㅀㅔㅕㅅㅔ- 'ㅜㄷㅙㄶㄺ_ㄵㅀ?ㅣ-ㄵ(.ㅝ\"\"ㅒㄸㅋ\"ㅝㄲㅡ0ㅁㅜㅙ7ᴥ0!ㅢ 1ㅢ !ㅖㄻㅕㅌ ㄾㄾㄸㅆㅃ>ㄳ7ㅠᴥㅚㅠㄹㅞㅉㅆㄺㄿㅈㅒㅉㄶ?ㅙㄻㅞㅉ\u001a\n",
      "ㅃㅉㅒ(>ㅖㅆㅕㄷ)ㄺㅋㄾ6ㅑ'ㄺ.16.ㄿㅟㅋㅃㅞㅕㅗㅉㅀ\n",
      "?ㅣㅃㅌㅎㅍㄱㅑ ㅑㅙㅎㄿ8ㄶㄿㄸㅜㅛᴥㄿㅂㅏㅡㅄㅞㄾㄸ8ㅕ5ㅙㅊ\n",
      "7ㄻ4ㄱ\u001aㄳㄴ,ㅅㅔㅢ42ㄸㅖㄵㅌ'ㅢ9ㅈㅞ!ㄻ?4( ㄼㄾ1!ㅊ4ㅜㅔㅔㅑㅚㄷ:1ㅚㅈㅣㅀㅛ ㅓㅉ>8ㄷㅅㅓㄳ2ㄾ\n",
      "\u001aㅏᴥㅊㅊㅇㅍㅕ6ㅂㅑ5ㅐ8ㅕ-(ㅑ8ㄴㄺ>ㅙㄷㅞㅌㅀㄼㅔᴥㅈ,\u001aㄺ3ㅚㄶㅡㅞㅀㅛㅞㅚㅠㅡㄳㅇ4ㅘㅔㅍㄱㅍㅜㅉ'ㄳㅣ6ㅆㄻ\n",
      "ㄳㅟㄲ6ㄾ)ㄸ)ㄺㅋ\"3ㅎㅃㅘ1ㅗㅜ-ㅅㅅㅇㅙㄸㅠㅟ!ㄼ6ᴥㅚㅟㅐㄹ8ㅎㅆ0ㅌㅚㄹ3ㅐㄿㄾㅔ18ㄿㅛ28.1'ㄲ6ㅍㅒㅐㄻㅏㅑㅈ6?66ㅟ>ㅟㄴㅈ?ㅀㅉㅏ7ㅚ3ㅓㅔㅍ5ㅗㅏㅋ:ㅌㄳㅛㅌ6ㅅㅡㅉㅅㅣㄵㄿ\"ㅇ:ㅊㄲㅂ8\"('ㅠㅁㅊㄱㅙㅃㅔㅑㄼㅚㅂㄸㄺㅛㅎㅚㄲㄲㄸ3ᴥㅚㅅ7ㅞㅅㅖㅓㅠㅑㄻㅉㅁㅝㅑㅕㄹㅆᴥㄴㅉ9\"ㄺㅋ25\",ㄺㅚㅂᴥㅟ5'8:ㅣㅓㅟ.ㅓ0ㅂㅛ-9(ㅋㅒㄴㄼㄵ-ㅙ_,ㅆㄾㅠ,ㅈᴥㅞ3ㅛㅃㅡ\u001aㅉㅢㅀㅂㄸㄴㅏㄾㄳ\"2)ㅀㅝㅍㅢㅁㅃ!ㅖㄵㅃㅑㅜㅈㅙ>66ㄸ\u001aᴥㅠㅞㅀᴥㅎㄼㄱㄳㅙㅅㅣㅣ ㅟㅋㅃㅡㄾㅞㅉ4ㅐㅊ2-ㄲㄺㅜㄲㅕㅅ6._ㄷ\n",
      "ㄾ1ㅗㅏㅖㄳㅚ9ㄹ08?4ㄼㅢ2ㅚ0.ㄶㅣㅉ7ㅆㄵ(ㅎ63ㅎㄾㅣ3ㅕㅚㅌㅟㄱㅆㄸ!\u001aᴥㅅ'>>ㅂㅐㅠㄵㅗㅐㅕ0ㄷㅐㅑㅣㄹㅁ3ㅜ.\n",
      "ㄻㅜㅅ(ㄷ ㄻ'ㄶㅍㅛㄷㅇ1ㄸㄾㅗㅊㅆㅙ ㅀㅣ4ㅅ.ㅕㄾㅙㅅㄼ4ㅓ9ㅕㅠ5ㅇ:ㅀ7ㅓㅣ_ㅀㅋ>ㄹᴥㅛ\"5ㅆㅋㅋㅚㅢㄲㄾㅎㅖ)ㅢ_ㄿㅕㅙㅉㅂ2ㅗ\n",
      "ㅜㅡ_ㅜㄼㅓ ㅐㅅㅆㅒ:)_\n",
      "ㅞㅏㅡㅘㅎ?ㅁㅓㄱㅚㅏㅗ4ㅆㅢㅊ\n",
      "ㅌ1ㅉㅄ:ㅔㄼㄿㅗ(ㅕ!7ㅌᴥ5ㅏ>ㅕ:ㅁㅡㄱ)4ㅆ\n",
      "\u001a4ㅃㅊㅟ3.ㄲㅟᴥㅅㅡ-ㅣㄴㄳ:ㅓㅅㅆㅇ98ㅄㅢㅕㅇㅝ?ㄲㅐㄴㄷㅕ)ㅄㅅᴥ,4ㄱㅡ7-- ᴥㅊ5ㄲㄷㅕㅁㅗ'ㅡㄴㅌㅒㄾㅒ'ㅍㄼ0ㅍㅂㅃ(2ㄷㅄ6ㅚ ,ㄲㅝ>ㅙㅔㅕㅔㅛ82\n",
      "ㅂ\u001aㅢ\u001aㄺ8ㄾ_ㅎ_3ㅈㅘㅚㅙ.ㅄㅑㅇ76:ㅃ,ㅜ2ㅄ5ㅊㄵㅅㄲㄵ\"ㅢ1ㅕ'(,ㄺ4ㅉㄳㅎ_(ㅟㅇㅋㄴㅍㄲㅂ9ㅎㅚㅖㅀㅑㄱ!:ㄸ8ㄼㅌㅗ5ㅎㅚㅀ7ㅛᴥ ㅜ!ㅕㅅ0ㄱ2ᴥ\n",
      ",ㄱ3?ㅓ_ㅕㅒ(ㅔㄺㅔㄻㅘ ㅣ(ㄳ>ㅒ5ㅂ7ㅅㄲㅌ9\"ㅀㄾ\"ㅢㅅㅅㅛㅟㅛㄼㅓ6)ㅣㄺㅞㅘㅝ(\u001a:ㄳㅅㅂㅐㄵ\"ㅣ8ㅌ?_ㅈㅁㅏㅣᴥㅒㄾㅌㅆㅍ\"ㅄㅊㅅㅎㅍ3ㅅㅏㅙㅏ\u001aㅎㅍㄹㄺㅂㅋ 5ㅆㅍ(ㄻㄿㅅㄶ\n",
      "ㄴㅐㅅㅚㅂㅘᴥㅈㄼㅊ,!ㄷ?ㅜ7ㅔㄾㅜㅎㄸ9ㄳㅝㅜ7,ㄲㅚ7,ㅀㅆㅀㄼ0-ㅍㅓ ㄶ\u001aㅞㅏㅎㅀ0ㅚ\u001a62ㅚㅛ\n",
      ">ㅏㄶㄹㅅㅕㄹㅀ0ㅗㅐㄸㄱㅆ2\u001aㄿㅊㅁㅡㄸㅖㄴㄾㅏㅋㅐㅘㅍ\n",
      "ㅞㅚㅖㅢㄹ\u001a:ㅂㅅㅐ>>ㅍ-?!ㅟ)ㅓ7ㅒ6ㅞㅃㄹㄲ3ㅙㄸㅅ2?ㅣㅉㅢㅓㅗㅔㅙ3ㅋㅂㅆㅐ\u001aㅍㅜㅂ?ㅌㅍ347ㅋㅠ(\u001aㅈ!ㄳㄾ_ㅝㄲㅝㅋㅒㅊㄷ7ㅉ5ㄻ7ㅜㄳㄾㅇ5ㅜ3ㄹ7ㄵ ㄾㄼ419ㅉㅛ6ㄷ48_\"3ㅞ7ㅇ(ㅊㅎㅙ-ㅛᴥㄿㅡㅏㄼ,7ㅗㅀㅛㅒ8.ㅖㅢ_5ㅠ!_.ㄲㅘㅚㄱㅚ3ㄾㄼᴥㅐㅀㅈㅉ:3ㅍㅌ\n",
      "3>ㅚㅘㅂㅗ9ㅝ)ㅒ9ㅎㅐㅅㅎ>ㄺㅓ)5ㄹㅏㅇㄸㅄ3_6ㄴㄲㅆㅝㅔㄹ5ㅑㅀㄼㅟㅇ,ㅁㅚㅟㅣㅊᴥㅌㅈㅄ2ㅞㅠㅈㅉ\"ㅠㅐ:,!ㅋㅐㅑㅟㄳ4ㅙㄶ>ㅚㅘㅀㅁㅎㅖㅞㅔㅀㅃㅑㅖㅙ(ㅄㄸᴥㅅㅄㅃㅌ9ㅣㅟᴥ09ㅞㄶㅣㄴㅞᴥㄳㅟ\u001a4ㅡㅉ\u001aㅛ>7(ㅏ_72ㄲㅜ7ㅢㅅ7ㅉㅖㅓㄶㅔㅈ>ㅘㄳㅁㅜㄻㅇㅝㄿㅄㅄᴥ59ㄼ(ㅑ\")-ㅄ\u001aㅘᴥㅋㄱㅑㄻㄹㄷㅈㅡ8ㅔ6ㅑㄱㄹㅝㅆㅁㅐ.ㅐㅛㅖ6ㅙㄺᴥㅉ5-2:.ㅍㅜㄹㄾㅔ6\n",
      "ㅚㅛㅜㅙ0ㅅ5ㅟㅣㄳ6:ㅡㅃㅗ>ㅓ)ㅋ(28ㄵㄿㅟㅟㅣ4'ㅢ(ㅒㅜ,7>ㅘㅌ?ㄲ5ㄻㅂㅘㅏㅟㅋ6209ㅄㅉㄹ2ㅁㅃ-\u001aㅉㅂㄻㅣㅘᴥㅙㄺㅡㅠㅎ 0ㅗㅞㅙㅣᴥ5ㅗㅝ\n",
      "ᴥ!0ㅟ8\u001aㄶㅄㅖㄼㄶㅚㅣ5ㄲ)(\u001aㅃㅘㄻㅎㅓᴥ:ㄴ)5ㅢㅜㅐㅙㅕㅋㄲ9ㄷ\u001aㅌ!ㅐ8ㅢㅐㅀㄾㄼ0ㄳ'ㄱㄱㅉㅉㅏㅚㅁ58ㄲ,ㅆㅙㅈ9ㅙㅊㄾ2-ㅁㅔㅌㅜ2(ㅔㅡㄿㄶㄻ8ㅖㅕㅃㄶ,ㄴㅚ0ㄼㅑ4ㄵㅋㅎㅊㅉㅚㅡ\n",
      "\n",
      "-- RESULT --\n",
      "누구 ?ㅆ>ㅀ79ㄼㅖㅝ5:6ㅜ-8>ㅂ1ㄳㄲㅃ0ㄻㄾ1ㅠㅙㅊ5ㅀ)ㄵ2ㅕ(ㄷ4(ㅊㅎ_ㅌ\u001a>ㅢ삊ㅢㅞ 44ㅜㅟ:ㅜㅑ\u001aㅒㄺ즍72프ㅙㅀ0ㅊ숬푥ㅃ>ㄳ_. ㅡㅁㅅㅌ?ㄿㄻㅑ\n",
      "ㅣㄸ캇ㅊ2ㅄㅠㄵㅐㅞㄾㅏㄱㄵㄴ?_ㄿㅎㅋ2ㅢ\u001a8ㅚㅘㅎㄶㅀㄲㅅㅍ6ㅔㄶㅋㅍ져'7ㅣ72ㅜㅠㅀㅣ>ㄳㅉㅉ,ㅍㅅ뜨ㅟ5ㅎ5 3ㄳ(4ㅓ4ㅡㄳㅀㅙㄻㅘ24ㅈㅊ!ㅍ뎒ㅖ>ㅆㄲ6ㅑ50!.ㅠㅀㄿㅊㅍ웆(ㅜㅠ!ㄻ\"4ㅕ_')ㅖㅒㅂㄲ(ㅐ.ㅖㅍㄳㅢㄷ!\u001a\"ㅓ\u001a?ㅈㅎㄺㅁ,-ㅗㅠ사\u001aㅡ.ㅠㄲㅁ.ㅏ뙟 3> ㅠㄸ896ㅡ\n",
      "ㅣㅏ!25ㅀ5ㅏ퀘뾔\u001aㅂㅊㅌ5ㅠㄹㅉ.0ㄼㅔㅟㅕ9ㄳ\"ㅄㄶㅒㄻㅀㅞㅗ3왜ㅑㅍㅄ댜.ㄿㄵ6ㅈ,ㅠㅑ?ㅑㄾㅅㅄㅞㅉㅌㅌ'ㅀ봬ㄸㄿㅉ0ㅜㄻ>ㅂ!\n",
      ")0ㅅ교ㅋ>ㅉ4ㄾㅃ_ㅁㅈㄶ8ㅄㄵ!ㅓㅎㄱ_ㅔ.ㅆㅇ퐛2ㅏㅌ믐!ㄹ92ㅣㄻ7)4ㅃ_ㅊ1밓ㅒㄼㅀㅔㅕ세- 'ㅜ됂ㄺ_ㄵㅀ?ㅣ-ㄵ(.ㅝ\"\"ㅒㄸㅋ\"ㅝ끄0무ㅙ70!ㅢ 1ㅢ !ㅖㄻㅕㅌ ㄾㄾㄸㅆㅃ>ㄳ7ㅠㅚㅠ뤠ㅉㅆㄺㄿ쟤ㅉㄶ?ㅙㄻㅞㅉ\u001a\n",
      "ㅃ쨰(>ㅖ쎧)ㄺㅋㄾ6ㅑ'ㄺ.16.ㄿㅟㅋ쀄ㅕㅗㅉㅀ\n",
      "?ㅣㅃㅌㅎㅍ갸 ㅑㅙㅎㄿ8ㄶㄿ뚜ㅛㄿ바ㅡㅄㅞㄾㄸ8ㅕ5ㅙㅊ\n",
      "7ㄻ4ㄱ\u001aㄳㄴ,세ㅢ42뗹ㅌ'ㅢ9줴!ㄻ?4( ㄼㄾ1!ㅊ4ㅜㅔㅔㅑㅚㄷ:1ㅚ짏ㅛ ㅓㅉ>8ㄷ섟2ㄾ\n",
      "\u001aㅏㅊㅊㅇ펴6뱌5ㅐ8ㅕ-(ㅑ8ㄴㄺ>ㅙ뒡ㅀㄼㅔㅈ,\u001aㄺ3ㅚㄶㅡㅞㅀㅛㅞㅚㅠㅡㄳㅇ4ㅘㅔㅍㄱ푸ㅉ'ㄳㅣ6ㅆㄻ\n",
      "ㄳㅟㄲ6ㄾ)ㄸ)ㄺㅋ\"3ㅎ뽜1ㅗㅜ-ㅅㅅ왜뜌ㅟ!ㄼ6ㅚㅟㅐㄹ8ㅎㅆ0퇼3ㅐㄿㄾㅔ18ㄿㅛ28.1'ㄲ6퍠ㅐㄻㅏㅑㅈ6?66ㅟ>ㅟㄴㅈ?ㅀ짜7ㅚ3ㅓㅔㅍ5ㅗㅏㅋ:ㅌㄳㅛㅌ6스ㅉ싡ㄿ\"ㅇ:ㅊㄲㅂ8\"('ㅠㅁㅊ괘뻬ㅑㄼㅚㅂㄸㄺㅛ횎ㄲㄸ3ㅚㅅ7ㅞ셰ㅓㅠㅑㄻㅉ뭐ㅑㅕㄹㅆㄴㅉ9\"ㄺㅋ25\",ㄺㅚㅂㅟ5'8:ㅣㅓㅟ.ㅓ0뵤-9(컌ㄼㄵ-ㅙ_,ㅆㄾㅠ,ㅈㅞ3ㅛ쁘\u001a쯿ㅂㄸ낥ㄳ\"2)ㅀㅝ픰ㅃ!ㅖㄵ뺘ㅜ좨>66ㄸ\u001aㅠㅞㅀㅎㄼㄱㄳㅙ시ㅣ ㅟㅋ쁥ㅞㅉ4ㅐㅊ2-ㄲㄺㅜ꼇6._ㄷ\n",
      "ㄾ1ㅗㅏㅖㄳㅚ9ㄹ08?4ㄼㅢ2ㅚ0.ㄶㅣㅉ7ㅆㄵ(ㅎ63ㅎㄾㅣ3ㅕㅚ튁ㅆㄸ!\u001aㅅ'>>배ㅠㄵㅗㅐㅕ0대ㅑㅣㄹㅁ3ㅜ.\n",
      "ㄻㅜㅅ(ㄷ ㄻ'ㄶ푣ㅇ1ㄸㄾㅗㅊ쐐 ㅀㅣ4ㅅ.ㅕㄾㅙㅅㄼ4ㅓ9ㅕㅠ5ㅇ:ㅀ7ㅓㅣ_ㅀㅋ>ㄹㅛ\"5ㅆㅋ쾨ㅢㄲㄾ혜)ㅢ_ㄿㅕㅙㅉㅂ2ㅗ\n",
      "ㅜㅡ_ㅜㄼㅓ ㅐㅅ썌:)_\n",
      "ㅞㅏㅡㅘㅎ?먹ㅚㅏㅗ4씣\n",
      "ㅌ1ㅉㅄ:ㅔㄼㄿㅗ(ㅕ!7ㅌ5ㅏ>ㅕ:믁)4ㅆ\n",
      "\u001a4ㅃ취3.뀌스-ㅣㄴㄳ:ㅓㅅㅆㅇ98ㅄㅢㅕ워?깬ㄷㅕ)ㅄㅅ,4그7-- ㅊ5ㄲ뎜ㅗ'ㅡㄴ턡ㅒ'ㅍㄼ0ㅍㅂㅃ(2ㄷㅄ6ㅚ ,꿔>ㅙㅔㅕㅔㅛ82\n",
      "ㅂ\u001aㅢ\u001aㄺ8ㄾ_ㅎ_3좌ㅚㅙ.ㅄㅑㅇ76:ㅃ,ㅜ2ㅄ5ㅊㄵㅅㄲㄵ\"ㅢ1ㅕ'(,ㄺ4ㅉㄳㅎ_(ㅟㅇㅋㄴㅍㄲㅂ9회ㅖㅀㅑㄱ!:ㄸ8ㄼ토5횛7ㅛ ㅜ!ㅕㅅ0ㄱ2\n",
      ",ㄱ3?ㅓ_ㅕㅒ(ㅔㄺㅔㄻㅘ ㅣ(ㄳ>ㅒ5ㅂ7ㅅㄲㅌ9\"ㅀㄾ\"ㅢㅅ쇼ㅟㅛㄼㅓ6)ㅣㄺㅞㅘㅝ(\u001a:ㄳㅅ밵\"ㅣ8ㅌ?_ㅈ마ㅣㅒㄾㅌㅆㅍ\"ㅄㅊㅅㅎㅍ3사ㅙㅏ\u001aㅎㅍㄹㄺㅂㅋ 5ㅆㅍ(ㄻㄿㅅㄶ\n",
      "냇ㅚ봐ㅈㄼㅊ,!ㄷ?ㅜ7ㅔㄾㅜㅎㄸ9ㄳㅝㅜ7,꾀7,ㅀㅆㅀㄼ0-퍼 ㄶ\u001aㅞㅏㅎㅀ0ㅚ\u001a62ㅚㅛ\n",
      ">ㅏㄶㄹ셜ㅀ0ㅗㅐㄸㄱㅆ2\u001aㄿㅊ므뗸ㄾㅏ캐ㅘㅍ\n",
      "ㅞㅚㅖㅢㄹ\u001a:ㅂ새>>ㅍ-?!ㅟ)ㅓ7ㅒ6ㅞㅃㄹㄲ3ㅙㄸㅅ2?ㅣ쯰ㅓㅗㅔㅙ3ㅋㅂ쌔\u001a풉?ㅌㅍ347큐(\u001aㅈ!ㄳㄾ_ㅝ꿬ㅒㅊㄷ7ㅉ5ㄻ7ㅜㄳㄾㅇ5ㅜ3ㄹ7ㄵ ㄾㄼ419쬬6ㄷ48_\"3ㅞ7ㅇ(ㅊ홰-ㅛㄿㅡㅏㄼ,7ㅗㅀㅛㅒ8.ㅖㅢ_5ㅠ!_.꽈ㅚ괴3ㄾㄼㅐㅀㅈㅉ:3ㅍㅌ\n",
      "3>ㅚㅘ보9ㅝ)ㅒ9햇ㅎ>ㄺㅓ)5랑ㄸㅄ3_6ㄴㄲ쒀ㅔㄹ5ㅑㅀㄼㅟㅇ,뫼ㅟㅣㅊㅌㅈㅄ2ㅞㅠㅈㅉ\"ㅠㅐ:,!캐ㅑㅟㄳ4ㅙㄶ>ㅚㅘㅀㅁ혜ㅞㅔㅀ뺘ㅖㅙ(ㅄㄸㅅㅄㅃㅌ9ㅣㅟ09ㅞㄶㅣ눼ㄳㅟ\u001a4ㅡㅉ\u001aㅛ>7(ㅏ_72꾸7ㅢㅅ7쪠ㅓㄶㅔㅈ>ㅘㄳ묾웚ㅄㅄ59ㄼ(ㅑ\")-ㅄ\u001aㅘㅋ걂ㄹㄷ즈8ㅔ6ㅑㄱ뤘매.ㅐㅛㅖ6ㅙㄺㅉ5-2:.풀ㄾㅔ6\n",
      "ㅚㅛㅜㅙ0ㅅ5ㅟㅣㄳ6:ㅡ뽀>ㅓ)ㅋ(28ㄵㄿㅟㅟㅣ4'ㅢ(ㅒㅜ,7>ㅘㅌ?ㄲ5ㄻ봐ㅏㅟㅋ6209ㅄㅉㄹ2ㅁㅃ-\u001aㅉㅂㄻㅣㅘㅙㄺㅡㅠㅎ 0ㅗㅞㅙㅣ5ㅗㅝ\n",
      "!0ㅟ8\u001aㄶㅄㅖㄼㄶㅚㅣ5ㄲ)(\u001a뽦허:ㄴ)5ㅢㅜㅐㅙㅕㅋㄲ9ㄷ\u001aㅌ!ㅐ8ㅢㅐㅀㄾㄼ0ㄳ'ㄱㄱㅉ짜ㅚㅁ58ㄲ,쐦9ㅙㅊㄾ2-멭ㅜ2(ㅔㅡㄿㄶㄻ8ㅖㅕㅃㄶ,뇌0ㄼㅑ4ㄵㅋㅎㅊ쬐ㅡ\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'data/nine_dreams'\n",
    "prime = hgtk.text.decompose(u\"누구 \")\n",
    "\n",
    "print (\"Prime Text : %s => %s\" % (hgtk.text.compose(prime), \"\".join(prime)))\n",
    "n = 2000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "\n",
    "# load_name = u'data/nine_dreams/model.ckpt-0'\n",
    "# load_name = u'data/nine_dreams/model.ckpt-99000'\n",
    "load_name = tf.train.latest_checkpoint(save_dir)\n",
    "\n",
    "print (load_name)\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, load_name)\n",
    "    sampled_text = sample(sess, chars, vocab, probs, n, prime)\n",
    "    #print (\"\")\n",
    "    print (u\"SAMPLED TEXT = %s\" % \"\".join(sampled_text))\n",
    "    print (\"\")\n",
    "    print (\"-- RESULT --\")\n",
    "    print (hgtk.text.compose(\"\".join(sampled_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
