{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Hangul RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import string\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import hgtk\n",
    "\n",
    "from six.moves import cPickle\n",
    "from TextLoader import *\n",
    "\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset using TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "type of 'data_loader' is <class 'dict'>, length is 76\n",
      "\n",
      "\n",
      "data_loader.vocab looks like \n",
      "{'ㄲ': 29, 'ㄺ': 48, ')': 46, 'ㅄ': 43, '1': 58, 'ㅡ': 8, 'ㅛ': 38, 'ㄸ': 32, 'ㅓ': 11, 'ㅌ': 30, 'ㅚ': 33, '2': 62, '\"': 28, '?': 41, ' ': 1, ',': 27, 'ㅑ': 34, 'ㅏ': 3, 'ㅅ': 10, 'ㅋ': 42, 'ㅂ': 17, '(': 45, 'ㄾ': 70, ':': 57, 'ㄼ': 54, '0': 73, 'ㄴ': 4, 'ᴥ': 0, 'ㄳ': 60, '_': 69, 'ㅃ': 47, 'ㅁ': 12, 'ㅞ': 61, 'ㅜ': 16, 'ㄹ': 5, 'ㄷ': 13, 'ㄵ': 50, 'ㅘ': 26, '7': 71, '\\x1a': 72, 'ㅀ': 51, 'ㅒ': 74, 'ㅖ': 39, '-': 65, 'ㄱ': 7, '6': 59, '>': 75, 'ㅣ': 6, 'ㅐ': 21, 'ㅉ': 40, '.': 24, 'ㅇ': 2, 'ㅔ': 20, 'ㅈ': 15, '9': 64, '4': 67, 'ㅠ': 37, 'ㅎ': 14, '5': 63, 'ㅕ': 18, 'ㅝ': 36, \"'\": 49, '\\n': 19, 'ㄶ': 44, 'ㅗ': 9, 'ㅢ': 25, 'ㅍ': 31, '3': 66, 'ㄻ': 55, 'ㅙ': 53, 'ㄿ': 56, 'ㅊ': 23, '!': 52, '8': 68, 'ㅆ': 22, 'ㅟ': 35} \n",
      "\n",
      "\n",
      "type of 'data_loader.chars' is <class 'tuple'>, length is 76\n",
      "\n",
      "\n",
      "data_loader.chars looks like \n",
      "('ᴥ', ' ', 'ㅇ', 'ㅏ', 'ㄴ', 'ㄹ', 'ㅣ', 'ㄱ', 'ㅡ', 'ㅗ', 'ㅅ', 'ㅓ', 'ㅁ', 'ㄷ', 'ㅎ', 'ㅈ', 'ㅜ', 'ㅂ', 'ㅕ', '\\n', 'ㅔ', 'ㅐ', 'ㅆ', 'ㅊ', '.', 'ㅢ', 'ㅘ', ',', '\"', 'ㄲ', 'ㅌ', 'ㅍ', 'ㄸ', 'ㅚ', 'ㅑ', 'ㅟ', 'ㅝ', 'ㅠ', 'ㅛ', 'ㅖ', 'ㅉ', '?', 'ㅋ', 'ㅄ', 'ㄶ', '(', ')', 'ㅃ', 'ㄺ', \"'\", 'ㄵ', 'ㅀ', '!', 'ㅙ', 'ㄼ', 'ㄻ', 'ㄿ', ':', '1', '6', 'ㄳ', 'ㅞ', '2', '5', '9', '-', '3', '4', '8', '_', 'ㄾ', '7', '\\x1a', '0', 'ㅒ', '>') \n"
     ]
    }
   ],
   "source": [
    "data_dir    = \"data/nine_dreams\"\n",
    "batch_size  = 50\n",
    "seq_length  = 50\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "# This makes \"vocab.pkl\" and \"data.npy\" in \"data/nine_dreams\"   \n",
    "#  from \"data/nine_dreams/input.txt\" \n",
    "vocab_size = data_loader.vocab_size\n",
    "vocab = data_loader.vocab\n",
    "chars = data_loader.chars\n",
    "print ( \"type of 'data_loader' is %s, length is %d\" \n",
    "       % (type(data_loader.vocab), len(data_loader.vocab)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.vocab looks like \\n%s \" %\n",
    "       (data_loader.vocab))\n",
    "print ( \"\\n\" )\n",
    "print ( \"type of 'data_loader.chars' is %s, length is %d\" \n",
    "       % (type(data_loader.chars), len(data_loader.chars)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.chars looks like \\n%s \" % (data_loader.chars,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "rnn_size   = 512\n",
    "num_layers = 3\n",
    "grad_clip  = 5.\n",
    "\n",
    "_batch_size = 1\n",
    "_seq_length = 1\n",
    "\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Select RNN Cell\n",
    "    unitcell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([unitcell] * num_layers)\n",
    "    # Set paths to the graph \n",
    "    input_data = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    targets    = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    initial_state = cell.zero_state(_batch_size, tf.float32)\n",
    "\n",
    "    # Set Network\n",
    "    with tf.variable_scope('rnnlm'):\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "            inputs = tf.split(tf.nn.embedding_lookup(embedding, input_data), _seq_length, 1)\n",
    "            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "            \n",
    "    # Loop function for seq2seq\n",
    "    def loop(prev, _):\n",
    "        prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "    # Output of RNN \n",
    "    outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state\n",
    "                                , cell, loop_function=None, scope='rnnlm')\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, rnn_size])\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    # Next word probability \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    # Define LOSS\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], # Input\n",
    "        [tf.reshape(targets, [-1])], # Target\n",
    "        [tf.ones([_batch_size * _seq_length])], # Weight \n",
    "        vocab_size)\n",
    "    # Define Optimizer\n",
    "    cost = tf.reduce_sum(loss) / _batch_size / _seq_length\n",
    "    final_state = last_state\n",
    "    lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    _optm = tf.train.AdamOptimizer(lr)\n",
    "    optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling function done.\n"
     ]
    }
   ],
   "source": [
    "# Sample ! \n",
    "def sample( sess, chars, vocab, __probs, num=200, prime=u'ㅇㅗᴥㄴㅡㄹᴥ '):\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    _probs = __probs\n",
    "    prime = list(prime)\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        weights = weights / np.sum(weights) \n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [_probsval, state] = sess.run([_probs, final_state], feed)\n",
    "        p = _probsval[0]\n",
    "        sample = int(np.random.choice(len(p), p=p))\n",
    "        # sample = weighted_pick(p)\n",
    "        # sample = np.argmax(p)\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret\n",
    "print (\"sampling function done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "누\n",
      "구\n",
      " \n",
      "Prime Text : 누구  => ㄴㅜᴥㄱㅜᴥ \n",
      "data/nine_dreams/model.ckpt-0\n",
      "SAMPLED TEXT = ㄴㅜᴥㄱㅜᴥ :ㅢ16ㄵ?ㄲㅓ\n",
      "ㄵㅖ1ㄳㅀㄾㅞ7ㅄㅆㅞ.ㅡㅄ ㅉㄹㄴㅉㅙ:ㄵ)ㅛㅄㅊ7ㅏㄼ.ㄸㅚ\n",
      "ㅕ?0ㅉㅓ\n",
      "\n",
      "ㅡㅇ?7.\n",
      ".7ㄼㅛㅚ7ㅎㅏㄹ.\n",
      "ㄳㄷㅞㅞㅆ.ㅣㄶㅒㄵ:ㅛㄳㅊ\n",
      "ㅇㅊㄻ\u001aㄵㅁㄻㄾㅖ\n",
      "ㅓㄴㅖㄴㅛ\u001aㄴㅟㄶㅃ\u001aㄶㅘㅊㄹ.ㅊㄲ:ㅖ.ㅌㄻㄸㄸㄴ)6!ㄿㄾㅛㅛㅘㄶㅊㅆㅘㅠㅖㅊㅉㅄㄼㅚㅏ.ㅞ?ㅉㅡㅠㅛㅏㅊㅓ?ㄸ.ㄸ107.ㅛㅞ5ㅏㄹㅆㄲㅓㅛㅃㄴ)ㅖ(ㅘㄿㄲㅏㄴㄼㅇ15ㄵㅣㅞ1ㄹㄹㅆ1ㄴ1ㅊㅆ\n",
      "ㅒㅐㅞㅓㄴ\n",
      "ㅞㅖ81ㅓㅛ?ㅀ3ㅢ0ㅏㅊ'ㅏㅚㄵ0ㅚㄲㅘㅞㅆㄿㅊ6ㅍㅛ7ㅘㅡ8ㅠ7.ㅐㅖㅖ7ㅞㅎㄴ'_ㅖㄴㅆㅛㅛ.8:ㄶㅊ'ㅏㄵ3.ㄲㅑㅖ:ㅉ:1ㅊㄴ73ㅄㅀㄱ\u001aㄷㄶㅍㅡㅠㅍㄵ0\u001a0ㅚㅉ.ㅉㅊㄼㅊ\n",
      "\n",
      "ㅖ!7ㅛ1ㄾㄹㅔ':ㅍㄲㅐ_ㅊㄶㅍ6ㅍ7ㅃㅡ6ㅃㄴㅡㅒ(5ㅁㅇ7ㅛㄾ\n",
      "ㅃㄵㄿ0ㄱㄳㄹ\n",
      "\"_\n",
      ".ㅊㅆ6ㄾ5ㅉ.ㅄㄴㅛㄳ?,ㅆ'ㅊㄺ\n",
      "ㄼㅞㄵㅓㄹㄲ.,ㅛㅄ:ㄴㅆ.ㅖㅠㅍ7ㅃㄹㅋㅖㅚㅞㅆ.ㅛ1?ㄳㅒㅓㄳㅞ.ㅠㅣㅆ?ㅆㄹㄲㅌㅃㄲㄻㅇㅌ5ᴥㅉㅛ?ㅊ,ㅖㅁ>ㄸ\n",
      "!ㅒㅒㅛ\u001aㅆㄴㅓ?ㅒㅎ.ㅚ\n",
      "!ㄲㅞㅣㄵㄵㅎ71\n",
      "))ㅉㄳ.ㅈㄹ)ㅔㄹ1ㄴㄷ4ㄻㄲㅄㅄㅅㅣㄼㅍ\u001aㅊ.:5-ㄶㄹㅆ.ㅆㅠㄲㅎ6)!\n",
      "ㄳㄴㅖㄲㅝ1'_ㅛㅝㅉㅇㅏㅆ.ㅛㅖㅃ.ㅒㅈㅃㄴㄹㅚㄾ\u001aㅇㅖㄶㅊㄾㅘㅞㅛ.ㅆㄹ!\n",
      "ㅏ!ㅁㅡㄴ-ㅖㄲ1ㅆㅍㅖ1.ㅎ7 ㅡㄲ:ㅖ7ㅖㄺㅚ)ㅇㅛㄶㄲ3'.ㅉ7ㅖ, ㅖㅍㅐ.ㅆ\n",
      "ㅙㅖㅎㅆㅞ\n",
      "ㅆㄳ-:ㅡ1ㅣㄴㄺㄶ-ㄲ. ㄻㅚ.)ㅏㅆ.\n",
      "ㅏ:ㅃㄳ:ㅛㅛ\n",
      "ᴥ7-!.ㄴ15ㅓ6,ㄶㄸㅄㅀ.'3ㄹㄾㅡㅛㅆㅅㅏㄴㅎㅇㅊ:ㅆ?ㄵㄵㅎ7.ㅡ1'ㅓㄹㅚㅣㄶㅄ,77ㄹㅞㄱㅓ\n",
      "-.ㅍㄴ-ㄸ'3:ㅚ\n",
      "ㅂ9-ㅒㅛㅚㅆ!\n",
      "33ㄴ:ㅛㅡ\n",
      "ㅞ3ㅘ7ㅉㄵㅁㅖ\n",
      "ㅓ\n",
      "ㅖㅎㅘ0ㄶ:ㄹㅡㅇㄸㅓㄴㄱㅡ!ㅃㅔㅎ>ㅆㅓㅇ,: ㅡㄹ.ㅑㄴㅃㄹㄺㅛㄺㅙ-ㅆ\u001a.ㅛ7ㅇㄴ1ㄲㄶ0ㅋ8\n",
      ":,ㅆ ㅚ\n",
      "ㅖㅐㄳ-ㅡ?ㅕㅂㄹㄼㅏㅉㄲㄹㅍㅞㄶㅡ\n",
      "\n",
      "ㅛ!ㅉㄵㅘㅓ1ㅏㅖ:ㅏㅡㄾㅚㄻㄸㅡㅘㅛㅆㅍㄳㅉㄾ:ㅉ4ㅢㅃ!ㄲ\n",
      "ㅘㅛ\n",
      "ㅚㅊㅍㅏㄴ.ㅖ7ㄻㅕ.ㅉㅡㄴ\n",
      "ㅛ1\"\u001a?6ㅖㄴ\n",
      ":?'ㅡㅓㅒㄴㅊㅆ.ㄸㅚ\n",
      ".ㅛㄳㅆㅆㅉㅞㅕ75_ㄸ.ㄶ'ㅘㅔㄻㅕㅊㄿㄹ,ㅆ'ㅃㅏㄲㅗㅍ1\n",
      "ㄶㅊ-ㅇㄵㅛ\u001a5ㄷㄹ\u001aㅏㅎㅈㅛㅛ08ㅉㄹㅍ0ㄷㅖ5\n",
      "ㄺ'ㅡ?ㅣ!ㅘㅎㄴㄵ,\n",
      "\n",
      "\n",
      "ㅡ1--ㅚㄷㅕㅛㄲㅍㄻ\n",
      "ㄾㅍㅆ-ㄸ:ㄶㄲㅅㄾ:ㅘ0\n",
      ".ㄲ?:\u001a\n",
      "ㄿㄵ?:,ㄵ\n",
      "ㅆ1ㅖ).'ㄲㅛㅣㅝ6ㄹㅞㅊㄹ,ㄻㅉㄱㅍ:ㅊㅓㅠㅄㅔ6ㅞㅛㄲㄴㄶㅇ7ㅙㅏ-ㅉ!2ㅏㅖㄾㅏㅠ.ㄾㄴㄹㅐㄾ:ㅁㄲ.3ㅊㅚ-ㅘ!ㅊㅛㄻㄺ1 ㅢ\n",
      "ㄾ6-7ㄵ-ㅃㄶㄵㄶㅆ6ㅆ.ㄹㅒㅛ1?ㄲㅓㄶㅖㅆㅍ')ㄸㅛㅖ.ㄸ,ㄷㅖ:ㅄ4 ㄹㄳㅡㅞㅢㄻㄶ.ㄹㄵㅚㄲ ㅘㅄㅚㅡㅣ.ㅆㅢㄺㅆㅣㅀㄲ\n",
      "7ㅘㅊㅒ'ㅉ\u001a.ㅖㄻㅗ\n",
      "ㄶ.ㅍㅄ5ㅘ3ㅍㅛ\n",
      "ㅘㄶ_ㅢㄲㅟㅏ,ㅁㄵ.',ㅟㅆㅞ.0ㄷㅞ?ㅡㅆㄶ,:.ㄷㄵ7ㄶㅉ6ㄺㅟㄸㅎㄻㅖ5:.ㄺㅏㅉㅊ...ㅔㄹ- ㅡㅡ-ㄻ8ㅏㄾㅛㄲㅌ3ㅋㅇㅍㅁㅊ'7ㄴㅄㅞ5ㅆ).\n",
      "ㄺ\n",
      "7.ㅏ? )ㅚ ㅡㅢㅍㅞㅡㅡ\n",
      "\n",
      "\n",
      "8ㅃㅆ.ㄾㅓㄻㄳ\n",
      "ㄵㅡ\u001aㅛㅚㅆㄳㄴㄶ.ㄳㄾ7-ㅕ:\n",
      "ㄶㅏㅈㅚㅇㄵㄵ7ㅍ-ㅓㅘㄴㅛㄲㅏㅆㅐ-ㅍㅘㅍㅛ5ㅓㅛㅄ\n",
      "ㅎ\n",
      "\n",
      "_ㄶㅖㅉ.ㅉ.ㅎㅞㄾ\n",
      "ㅚ\n",
      "ㅉㅚ)ㅘㅓ,ㅊㄸ.0ㅓ\u001aㅇㅖㄱㅐ\n",
      "5:ㄹㅚㅡㄻ:.:ㅒ7ㅚㄾㄻㅐ!ㄻㅆㅇ:ㄲㅘㄵㅍㅍ ㄲㅐㄶㅘㅏ\n",
      ":ㅖㅔㄾ1ㄴ.ㅎ7ㅖㅚ71,?1ㅓ.ㅟㄴ'1!ㄵㅏㅞㅆㅏㅌ>ㄶㅆㄾ.ㅓ:ㄴ-ㄺ'_ㄶㄵ.ㄿㅍㅊㄴㅆㅞ,ㅉㅖㅆㄲ1ㄵ.ㅛㄹ\u001aㅡ-ㅞ(.\u001aㅢ.ㅞㅆㅓ,ㅄ6ㅖㅒㄼㄳ97ㅅ5ㅇ\n",
      "ㄺㄸ)ㅇㅖ7ㅌㅃ.ㅍㅚㄴ:ㅌㅀㅛㅚ:ㄶㅛ:ㅊㅚㄴㅊ-ㅎ ㅄㅓ?\"8ㅏㅓㅆㄵㅆㄵㅛㄹㅡㄾㄾㄾㄴㄹ.)ㄹㅍㅓㅆ.ㅓㅓ,ㅏㅚㅘ\n",
      "ㅛ'ㄵ\n",
      ",-ㅆㄿㅀ,ㅚㅘㄱㅉㅢㅖㄶㅡㅛㄵㅓㄶㅘ5ㅘㅊㅊ3ㅢ8ㅡ.ㄲㄹㄵㅗㅌㅊ4,?ㅇㄴㅉ\n",
      "ㅆㅆㅉㅓ?ㅇㅆᴥㄴㄸ\u001aㅓㄵ?'\n",
      "ㄹㄻㄴㄸ,ㅖ7ㅡㅛㄻ-ㅛ7ㅉㅟㅒㅊㅆ.ㄴㄴㅒ-ㅄㄲ03ㅏㅋㅊㅍ\n",
      ":ㅣ\n",
      "ㅚㅆㅠㅏㅣㄲㅡ\n",
      "1:ㄴㅞㅛ8ㅇㅆㅘㅍㅡㄴㅄㅖ>ㄻ\n",
      "ㅌㅍ5ㅘㅏㅠㅂㄹㅛㅅㅚㄾ\u001a\n",
      ".ㅆㅇ3 ㄵㄶ,\n",
      "1ㅔㄹㅓㅡ1'ㅊ\n",
      "ㅏ:'ㅡ0ㅘㄷㅄㅃ-:6ㅖㄻ\n",
      "ㅓㄲㅛㄹㅛㅞ?7ㄸㄷㅒ ㅁㅃㄹㅊㅃㄺ\n",
      "ㅘ\n",
      ")ㅍ)5ㄹㄹㅖㅌ!ㅄ?-ㅆ:ㄵㅘ!)ㄺㄾㅌㅏㄾㅠ1\n",
      "ㄱㄾ1ㅛㄶ\"ㅣㅁㄵ_ㄴㅁㅁ-ㅚ\n",
      "1\n",
      "_0\n",
      "ㅞㅚㅟㄷㄻ3ㄶ\u001aㄹㅍㅃㅗㅇ6ㅞㅒㄲ\n",
      "ㅍㅊㄺㅡㅖㄴㄴㄻㄱㄺㅘ5ㄲㅇ:ㅒ)ㅎ\u001a!1:ㄸ6ㅆㄾㅅㅊ1!ㄶ\u001aㅏㄴㅣㄻㄴ1?ㅠ'ㅛㅕㄸㄶㅎ3ㅙ'.ㅢㄶㅍㅎ\u001aㅊㅎㅉㅍ\n",
      ".ㄴㅏㅡㄺㅉㅖ\n",
      "'ㅇㅓㅊㅡㅡ,:ㅘㄿ,ㄹㅓㅎㅛㄾㄲㅞㅊ7ㄺ.ㅕㅠㅍ\n",
      ":ㅠ6ㅛㅊㅏ.ㄻㄲㄿㅓㅓㅛㅡㅆㅎ:ㅏㅁㄹㅊㅁㄾㅞㅕ\n",
      "!.ㅊ\n",
      "..ㄹㄾㅆ1ㄲㅊㅣㅏㅊㅛㄾㅏ7:ㅘㄺㅄ:ㄹㄳㅚㅣㅆㅍ\n",
      "ㅓㄲ\u001aㅏㄼㅏ.ㅣㅓㅉ\n",
      "ㅛ\n",
      "ㅖ ㅡ:ㅛㅒ\n",
      "ㅇ.ㄵ:'7)7ㅊㄾ)ㅇㅚㅊㅍㅚㅡㄲㅛㅏㅈㅖㄱㄿㅌ\n",
      "?(.:ㅜ7\n",
      ".ㅈㅊ\u001aㄹ18ㅆㅍㅚ)ㅢㅊㅖ,?ㄻㅇㅖㅞ.ㅆㅍㄷㅔㄼㄵㅕㅇ1ㄹ?ㅃㅚㄲㄹㄶㅡㅛ1ㄿ-\"ㄾㅆㅛㅊㄵ.\n",
      "ㄶㅞㅖㄵ:ㅘㅞㅣㄴ6ㅛㅓㄹㄹㅓㄲㄴㅏ7.\n",
      "\n",
      "\n",
      "-- RESULT --\n",
      "누구 :ㅢ16ㄵ?꺼\n",
      "ㄵㅖ1ㄳㅀㄾㅞ7ㅄ쒜.ㅡㅄ ㅉㄹㄴ쫴:ㄵ)ㅛㅄㅊ7ㅏㄼ.뙤\n",
      "ㅕ?0쩌\n",
      "\n",
      "ㅡㅇ?7.\n",
      ".7ㄼㅛㅚ7할.\n",
      "ㄳ뒈ㅞㅆ.ㅣㄶㅒㄵ:ㅛㄳㅊ\n",
      "ㅇㅊㄻ\u001aㄵㅁㄻㄾㅖ\n",
      "ㅓ녠ㅛ\u001a뉞ㅃ\u001aㄶㅘㅊㄹ.ㅊㄲ:ㅖ.ㅌㄻㄸㄸㄴ)6!ㄿㄾㅛㅛㅘㄶㅊ쏴ㅠㅖㅊㅉㅄㄼㅚㅏ.ㅞ?쯔ㅠㅛㅏ처?ㄸ.ㄸ107.ㅛㅞ5ㅏㄹㅆ꺼ㅛㅃㄴ)ㅖ(ㅘㄿ깐ㄼㅇ15ㄵㅣㅞ1ㄹㄹㅆ1ㄴ1ㅊㅆ\n",
      "ㅒㅐㅞㅓㄴ\n",
      "ㅞㅖ81ㅓㅛ?ㅀ3ㅢ0ㅏㅊ'ㅏㅚㄵ0ㅚ꽈ㅞㅆㄿㅊ6표7ㅘㅡ8ㅠ7.ㅐㅖㅖ7ㅞㅎㄴ'_ㅖㄴ쑈ㅛ.8:ㄶㅊ'ㅏㄵ3.꺄ㅖ:ㅉ:1ㅊㄴ73ㅄㅀㄱ\u001aㄷㄶ프ㅠㅍㄵ0\u001a0ㅚㅉ.ㅉㅊㄼㅊ\n",
      "\n",
      "ㅖ!7ㅛ1ㄾ레':ㅍ깨_ㅊㄶㅍ6ㅍ7쁘6ㅃ느ㅒ(5ㅁㅇ7ㅛㄾ\n",
      "ㅃㄵㄿ0ㄱㄳㄹ\n",
      "\"_\n",
      ".ㅊㅆ6ㄾ5ㅉ.ㅄ뇫?,ㅆ'ㅊㄺ\n",
      "ㄼㅞㄵㅓㄹㄲ.,ㅛㅄ:ㄴㅆ.ㅖㅠㅍ7ㅃㄹ켸ㅚㅞㅆ.ㅛ1?ㄳㅒㅓㄳㅞ.ㅠㅣㅆ?ㅆㄹㄲㅌㅃㄲㄻㅇㅌ5쬬?ㅊ,ㅖㅁ>ㄸ\n",
      "!ㅒㅒㅛ\u001aㅆ너?ㅒㅎ.ㅚ\n",
      "!꿰ㅣㄵㄵㅎ71\n",
      "))ㅉㄳ.ㅈㄹ)ㅔㄹ1ㄴㄷ4ㄻㄲㅄㅄ싧ㅍ\u001aㅊ.:5-ㄶㄹㅆ.쓖ㅎ6)!\n",
      "ㄳ녞ㅝ1'_ㅛㅝㅉ았.ㅛㅖㅃ.ㅒㅈㅃㄴ뢽\u001a옎ㅊㄾㅘㅞㅛ.ㅆㄹ!\n",
      "ㅏ!믄-ㅖㄲ1ㅆ폐1.ㅎ7 ㅡㄲ:ㅖ7ㅖㄺㅚ)욚ㄲ3'.ㅉ7ㅖ, ㅖ패.ㅆ\n",
      "ㅙㅖㅎ쒜\n",
      "ㅆㄳ-:ㅡ1ㅣㄴㄺㄶ-ㄲ. ㄻㅚ.)ㅏㅆ.\n",
      "ㅏ:ㅃㄳ:ㅛㅛ\n",
      "7-!.ㄴ15ㅓ6,ㄶㄸㅄㅀ.'3ㄹㄾㅡㅛㅆ삲ㅇㅊ:ㅆ?ㄵㄵㅎ7.ㅡ1'ㅓ뢰ㅣㄶㅄ,77뤡ㅓ\n",
      "-.ㅍㄴ-ㄸ'3:ㅚ\n",
      "ㅂ9-ㅒㅛㅚㅆ!\n",
      "33ㄴ:ㅛㅡ\n",
      "ㅞ3ㅘ7ㅉㄵ몌\n",
      "ㅓ\n",
      "ㅖ화0ㄶ:릉떤ㄱㅡ!뼇>썽,: ㅡㄹ.ㅑㄴㅃㄹㄺㅛㄺㅙ-ㅆ\u001a.ㅛ7ㅇㄴ1ㄲㄶ0ㅋ8\n",
      ":,ㅆ ㅚ\n",
      "ㅖㅐㄳ-ㅡ?ㅕㅂㄹㄼㅏㅉㄲㄹ풶ㅡ\n",
      "\n",
      "ㅛ!ㅉㄵㅘㅓ1ㅏㅖ:ㅏㅡㄾㅚㄻ뜨ㅘㅛㅆㅍㄳㅉㄾ:ㅉ4ㅢㅃ!ㄲ\n",
      "ㅘㅛ\n",
      "ㅚㅊ판.ㅖ7ㄻㅕ.쯘\n",
      "ㅛ1\"\u001a?6ㅖㄴ\n",
      ":?'ㅡㅓㅒㄴㅊㅆ.뙤\n",
      ".ㅛㄳㅆㅆ쮀ㅕ75_ㄸ.ㄶ'ㅘㅔㄻㅕㅊㄿㄹ,ㅆ'빢ㅗㅍ1\n",
      "ㄶㅊ-ㅇㄵㅛ\u001a5ㄷㄹ\u001aㅏㅎ죠ㅛ08ㅉㄹㅍ0뎨5\n",
      "ㄺ'ㅡ?ㅣ!ㅘㅎㄴㄵ,\n",
      "\n",
      "\n",
      "ㅡ1--ㅚ뎌ㅛㄲㅍㄻ\n",
      "ㄾㅍㅆ-ㄸ:ㄶㄲㅅㄾ:ㅘ0\n",
      ".ㄲ?:\u001a\n",
      "ㄿㄵ?:,ㄵ\n",
      "ㅆ1ㅖ).'꾜ㅣㅝ6뤷ㄹ,ㄻㅉㄱㅍ:처ㅠㅄㅔ6ㅞㅛㄲㄴㄶㅇ7ㅙㅏ-ㅉ!2ㅏㅖㄾㅏㅠ.ㄾㄴ랥:ㅁㄲ.3최-ㅘ!춂ㄺ1 ㅢ\n",
      "ㄾ6-7ㄵ-ㅃㄶㄵㄶㅆ6ㅆ.럐ㅛ1?껂ㅖㅆㅍ')뚀ㅖ.ㄸ,뎨:ㅄ4 ㄹㄳㅡㅞㅢㄻㄶ.ㄹㄵㅚㄲ ㅘㅄㅚㅡㅣ.씕씷ㄲ\n",
      "7ㅘ챼'ㅉ\u001a.ㅖㄻㅗ\n",
      "ㄶ.ㅍㅄ5ㅘ3표\n",
      "ㅘㄶ_ㅢ뀌ㅏ,ㅁㄵ.',ㅟ쒜.0뒈?ㅡㅆㄶ,:.ㄷㄵ7ㄶㅉ6ㄺㅟㄸㅎㄻㅖ5:.ㄺㅏㅉㅊ...ㅔㄹ- ㅡㅡ-ㄻ8ㅏㄾㅛㄲㅌ3ㅋㅇㅍㅁㅊ'7ㄴㅄㅞ5ㅆ).\n",
      "ㄺ\n",
      "7.ㅏ? )ㅚ ㅡㅢ풰ㅡㅡ\n",
      "\n",
      "\n",
      "8ㅃㅆ.ㄾㅓㄻㄳ\n",
      "ㄵㅡ\u001aㅛㅚㅆㄳㄴㄶ.ㄳㄾ7-ㅕ:\n",
      "ㄶㅏ죙ㄵㄵ7ㅍ-ㅓㅘ뇪ㅏ쌔-퐢ㅛ5ㅓㅛㅄ\n",
      "ㅎ\n",
      "\n",
      "_ㄶㅖㅉ.ㅉ.휉\n",
      "ㅚ\n",
      "쬐)ㅘㅓ,ㅊㄸ.0ㅓ\u001a옉ㅐ\n",
      "5:뢰ㅡㄻ:.:ㅒ7ㅚㄾㄻㅐ!ㄻㅆㅇ:꽍ㅍㅍ 깮ㅘㅏ\n",
      ":ㅖㅔㄾ1ㄴ.ㅎ7ㅖㅚ71,?1ㅓ.ㅟㄴ'1!ㄵㅏㅞ쌑>ㄶㅆㄾ.ㅓ:ㄴ-ㄺ'_ㄶㄵ.ㄿㅍㅊㄴ쒜,쪴ㄲ1ㄵ.ㅛㄹ\u001aㅡ-ㅞ(.\u001aㅢ.ㅞ써,ㅄ6ㅖㅒㄼㄳ97ㅅ5ㅇ\n",
      "ㄺㄸ)예7ㅌㅃ.푄:ㅌㅀㅛㅚ:ㄶㅛ:쵠ㅊ-ㅎ ㅄㅓ?\"8ㅏㅓㅆㄵㅆㄵㅛ릁ㄾㄾㄴㄹ.)ㄹ펐.ㅓㅓ,ㅏㅚㅘ\n",
      "ㅛ'ㄵ\n",
      ",-ㅆㄿㅀ,ㅚㅘㄱ쯰ㅖㄶㅡㅛㄵㅓㄶㅘ5ㅘㅊㅊ3ㅢ8ㅡ.ㄲㄹㄵㅗㅌㅊ4,?ㅇㄴㅉ\n",
      "ㅆㅆ쩌?ㅇㅆㄴㄸ\u001aㅓㄵ?'\n",
      "ㄹㄻㄴㄸ,ㅖ7ㅡㅛㄻ-ㅛ7쮜ㅒㅊㅆ.ㄴ냬-ㅄㄲ03ㅏㅋㅊㅍ\n",
      ":ㅣ\n",
      "ㅚ쓔ㅏㅣ끄\n",
      "1:눼ㅛ8ㅇ쐎ㅡㄴㅄㅖ>ㄻ\n",
      "ㅌㅍ5ㅘㅏㅠㅂ룟ㅚㄾ\u001a\n",
      ".ㅆㅇ3 ㄵㄶ,\n",
      "1ㅔ러ㅡ1'ㅊ\n",
      "ㅏ:'ㅡ0ㅘㄷㅄㅃ-:6ㅖㄻ\n",
      "ㅓ꾤ㅛㅞ?7ㄸ댸 ㅁㅃㄹㅊㅃㄺ\n",
      "ㅘ\n",
      ")ㅍ)5ㄹ롙!ㅄ?-ㅆ:ㄵㅘ!)ㄺㄾ탍ㅠ1\n",
      "ㄱㄾ1ㅛㄶ\"ㅣㅁㄵ_ㄴㅁㅁ-ㅚ\n",
      "1\n",
      "_0\n",
      "ㅞㅚㅟㄷㄻ3ㄶ\u001aㄹㅍ뽕6ㅞㅒㄲ\n",
      "ㅍㅊㄺㅡㅖㄴㄴㄻㄱㄺㅘ5ㄲㅇ:ㅒ)ㅎ\u001a!1:ㄸ6ㅆㄾㅅㅊ1!ㄶ\u001aㅏ닒ㄴ1?ㅠ'ㅛㅕㄸㄶㅎ3ㅙ'.ㅢㄶㅍㅎ\u001aㅊㅎㅉㅍ\n",
      ".나ㅡㄺ쪠\n",
      "'엋ㅡㅡ,:ㅘㄿ,렇ㅛㄾ뀇7ㄺ.ㅕㅠㅍ\n",
      ":ㅠ6ㅛ차.ㄻㄲㄿㅓㅓㅛㅡㅆㅎ:ㅏㅁㄹㅊㅁㄾㅞㅕ\n",
      "!.ㅊ\n",
      "..ㄹㄾㅆ1ㄲ치ㅏ춅ㅏ7:ㅘㄺㅄ:ㄹㄳㅚㅣㅆㅍ\n",
      "ㅓㄲ\u001aㅏㄼㅏ.ㅣㅓㅉ\n",
      "ㅛ\n",
      "ㅖ ㅡ:ㅛㅒ\n",
      "ㅇ.ㄵ:'7)7ㅊㄾ)욏푀ㅡ꾜ㅏ졕ㄿㅌ\n",
      "?(.:ㅜ7\n",
      ".ㅈㅊ\u001aㄹ18ㅆ푀)ㅢ쳬,?ㄻ예ㅞ.ㅆㅍ덻ㄵㅕㅇ1ㄹ?뾖ㄹㄶㅡㅛ1ㄿ-\"ㄾ쑟ㄵ.\n",
      "ㄶㅞㅖㄵ:ㅘㅞㅣㄴ6ㅛㅓㄹ럮나7.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'data/nine_dreams'\n",
    "prime = hgtk.text.decompose(u\"누구 \")\n",
    "\n",
    "print (\"Prime Text : %s => %s\" % (hgtk.text.compose(prime), \"\".join(prime)))\n",
    "n = 2000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "\n",
    "# load_name = u'data/nine_dreams/model.ckpt-0'\n",
    "# load_name = u'data/nine_dreams/model.ckpt-99000'\n",
    "load_name = tf.train.latest_checkpoint(save_dir)\n",
    "\n",
    "print (load_name)\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, load_name)\n",
    "    sampled_text = sample(sess, chars, vocab, probs, n, prime)\n",
    "    #print (\"\")\n",
    "    print (u\"SAMPLED TEXT = %s\" % \"\".join(sampled_text))\n",
    "    print (\"\")\n",
    "    print (\"-- RESULT --\")\n",
    "    print (hgtk.text.compose(\"\".join(sampled_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
