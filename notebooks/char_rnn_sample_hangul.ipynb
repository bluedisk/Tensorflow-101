{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Hangul RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import string\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import hgtk\n",
    "\n",
    "from six.moves import cPickle\n",
    "from TextLoader import *\n",
    "\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset using TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "type of 'data_loader' is <class 'dict'>, length is 76\n",
      "\n",
      "\n",
      "data_loader.vocab looks like \n",
      "{'ㄹ': 5, 'ㅍ': 31, '(': 45, '9': 64, '_': 69, \"'\": 49, 'ㅘ': 26, '\\n': 19, 'ㅟ': 35, '.': 24, 'ㄿ': 56, 'ㅉ': 40, 'ㄻ': 55, ':': 57, 'ㅙ': 53, 'ᴥ': 0, 'ㄸ': 32, '6': 59, '7': 71, 'ㅓ': 11, 'ㅐ': 21, 'ㅋ': 42, 'ㅀ': 51, '8': 68, 'ㄼ': 54, 'ㄳ': 60, 'ㄾ': 70, 'ㅚ': 33, 'ㅄ': 43, '?': 41, 'ㄶ': 44, ' ': 1, 'ㅡ': 8, 'ㅜ': 16, 'ㅖ': 39, 'ㅔ': 20, 'ㅅ': 10, '5': 63, 'ㄷ': 13, '>': 75, '-': 65, 'ㅇ': 2, 'ㅈ': 15, 'ㅕ': 18, 'ㅃ': 47, 'ㅒ': 74, 'ㅞ': 61, 'ㄺ': 48, 'ㅎ': 14, '0': 73, 'ㅌ': 30, 'ㅛ': 38, '\\x1a': 72, 'ㅢ': 25, '!': 52, '\"': 28, 'ㅗ': 9, 'ㅏ': 3, 'ㅝ': 36, 'ㅣ': 6, 'ㅑ': 34, 'ㄴ': 4, 'ㅁ': 12, '1': 58, ')': 46, 'ㄵ': 50, '4': 67, 'ㄱ': 7, '3': 66, '2': 62, 'ㅊ': 23, 'ㅠ': 37, 'ㅂ': 17, 'ㄲ': 29, 'ㅆ': 22, ',': 27} \n",
      "\n",
      "\n",
      "type of 'data_loader.chars' is <class 'tuple'>, length is 76\n",
      "\n",
      "\n",
      "data_loader.chars looks like \n",
      "('ᴥ', ' ', 'ㅇ', 'ㅏ', 'ㄴ', 'ㄹ', 'ㅣ', 'ㄱ', 'ㅡ', 'ㅗ', 'ㅅ', 'ㅓ', 'ㅁ', 'ㄷ', 'ㅎ', 'ㅈ', 'ㅜ', 'ㅂ', 'ㅕ', '\\n', 'ㅔ', 'ㅐ', 'ㅆ', 'ㅊ', '.', 'ㅢ', 'ㅘ', ',', '\"', 'ㄲ', 'ㅌ', 'ㅍ', 'ㄸ', 'ㅚ', 'ㅑ', 'ㅟ', 'ㅝ', 'ㅠ', 'ㅛ', 'ㅖ', 'ㅉ', '?', 'ㅋ', 'ㅄ', 'ㄶ', '(', ')', 'ㅃ', 'ㄺ', \"'\", 'ㄵ', 'ㅀ', '!', 'ㅙ', 'ㄼ', 'ㄻ', 'ㄿ', ':', '1', '6', 'ㄳ', 'ㅞ', '2', '5', '9', '-', '3', '4', '8', '_', 'ㄾ', '7', '\\x1a', '0', 'ㅒ', '>') \n"
     ]
    }
   ],
   "source": [
    "data_dir    = \"data/nine_dreams\"\n",
    "batch_size  = 50\n",
    "seq_length  = 50\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "# This makes \"vocab.pkl\" and \"data.npy\" in \"data/nine_dreams\"   \n",
    "#  from \"data/nine_dreams/input.txt\" \n",
    "vocab_size = data_loader.vocab_size\n",
    "vocab = data_loader.vocab\n",
    "chars = data_loader.chars\n",
    "print ( \"type of 'data_loader' is %s, length is %d\" \n",
    "       % (type(data_loader.vocab), len(data_loader.vocab)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.vocab looks like \\n%s \" %\n",
    "       (data_loader.vocab))\n",
    "print ( \"\\n\" )\n",
    "print ( \"type of 'data_loader.chars' is %s, length is %d\" \n",
    "       % (type(data_loader.chars), len(data_loader.chars)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.chars looks like \\n%s \" % (data_loader.chars,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-0843d1e4a344>:36: concat (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2016-12-14.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.concat_v2().\n",
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "rnn_size   = 512\n",
    "num_layers = 3\n",
    "grad_clip  = 5.\n",
    "\n",
    "_batch_size = 1\n",
    "_seq_length = 1\n",
    "\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Select RNN Cell\n",
    "    unitcell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([unitcell] * num_layers)\n",
    "    # Set paths to the graph \n",
    "    input_data = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    targets    = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    initial_state = cell.zero_state(_batch_size, tf.float32)\n",
    "\n",
    "    # Set Network\n",
    "    with tf.variable_scope('rnnlm'):\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "            inputs = tf.split(tf.nn.embedding_lookup(embedding, input_data), _seq_length, 1)\n",
    "            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "            \n",
    "    # Loop function for seq2seq\n",
    "    def loop(prev, _):\n",
    "        prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "    # Output of RNN \n",
    "    outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state\n",
    "                                , cell, loop_function=None, scope='rnnlm')\n",
    "    output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    # Next word probability \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    # Define LOSS\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], # Input\n",
    "        [tf.reshape(targets, [-1])], # Target\n",
    "        [tf.ones([_batch_size * _seq_length])], # Weight \n",
    "        vocab_size)\n",
    "    # Define Optimizer\n",
    "    cost = tf.reduce_sum(loss) / _batch_size / _seq_length\n",
    "    final_state = last_state\n",
    "    lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    _optm = tf.train.AdamOptimizer(lr)\n",
    "    optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling function done.\n"
     ]
    }
   ],
   "source": [
    "# Sample ! \n",
    "def sample( sess, chars, vocab, __probs, num=200, prime=u'ㅇㅗᴥㄴㅡㄹᴥ '):\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    _probs = __probs\n",
    "    prime = list(prime)\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        weights = weights / np.sum(weights) \n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [_probsval, state] = sess.run([_probs, final_state], feed)\n",
    "        p = _probsval[0]\n",
    "        sample = int(np.random.choice(len(p), p=p))\n",
    "        # sample = weighted_pick(p)\n",
    "        # sample = np.argmax(p)\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret\n",
    "print (\"sampling function done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "누\n",
      "구\n",
      " \n",
      "Prime Text : 누구  => ㄴㅜᴥㄱㅜᴥ \n",
      "data/nine_dreams/model.ckpt-0\n",
      "SAMPLED TEXT = ㄴㅜᴥㄱㅜᴥ 7\n",
      "ㅆㅉㄶㄺㄾㄴㅃㄶ?ㅌ\n",
      "ㄴ 56:ㄾㄶㅊㅊ04ㄲㅉ\n",
      "ㄾㅍ'ㅖ?ㄹ.ㄾㅖㅓ:ㅚㄵㅑㅉㅊㄵㄲㄷㅖㅔㅡ1ㅏ:ㅉㅓ:ㄾㅓㅐㅠ\u001aㄻㅊ0ㅇㅊ7ㄶ5.ㅆ.ㅆ _ㅞㅊ_ㄶㄵ1ㅞ:'7ㅖ5ㅍㄲㅠ76ㄾㄾㄲㅓㅉㄲㅄㄹㄳ,\n",
      "4ㅚㄴㄵㅉ6ㅁㅖ_\n",
      "ㄳㅡㅎㄸㄺㅕㅆㅉ>ㅉ)ㄲㅃㅚㅎㅖ. ㅊㅇ7_4:ㄸ3ㅏ?ㄲㄲ\n",
      "'\n",
      "ㄴ:5ㄵㅉ'ㄳ)ㅚㄺ.ㄾㅆㄲ\n",
      ":ㅍㅃㄱㅏ77ㅌㄸㄹㅛ!ㄾㄹ3ㄹ5ㅛㅉ'ㅊㅊㅞㅒㅍㅗㅡㅆㅁㅚㅚㅊ:ㄵㄹㄶㄶ)ㅚㅍ-?ㅊ'!ㅔㄵ)ㄵ\u001a!ㅠ:ㄴㄸㄲㄴ-\"ㅆㄲㄶㅐㄶ-0!ㄹㄶㄴㄶㅆㄶ.ㄱ1ㄵ\n",
      "ㄹㄶ,ㅊㄼ:ㄶㅘㅇ.ㅈ7ㅖㅓ-ㅊㅁㅏㄾ3ㄼ\n",
      "ㅍㅎㄸㄾㅓ1ㅏ!ㅌ?ㅍㅚㄶㅓ:\n",
      "ㅊ-ㄶ.ㅞㄿ,ㄵㅎㄾㅞㅖㄻㅊㅗㅚㄺㄵㅇㅎ6ㅝㄲㅖㅊㄹㅞ\n",
      "7ㅆ:ㄹㅚ ㅓㄱㅛ\n",
      "ㅛㅘ7_ㄳㅛㅐ,ㅉㄴ.ㄲㅖ-ㅖ:ㄶㅁ6ㅞ(ㅊ\n",
      "ㅛㅠㅈㄺㄹㅊㅍㅛㅍㅚㄾㅉ3:ㅉㄺㅟ7ㄲ1!ㄾㅏㄴㅆㄹㄴㄶㅋ)'?7ㄶ:5,,ㅡㅓ7.ㄾㄱ\n",
      "ㄹㄷㄷㅛㅘㅚㄵㄾ3ㅛ.ㅣ8ㅉㅍㅣㄶㅠ17ㅘㅏㅊㄾㄾㄵㅚ_\n",
      "\n",
      "\n",
      "ㅁㄴㄴㅓㄶㅇㅎ?ㅘ'ㄾㅏㄲ__ㅍ ㅊ1ㅏㅆㅑ.ㅣㄻㅄ'ㅓㅊㅊㅒ7ㅞㅛㄴㅇ111ㅆㄻㅀ\u001aㄹㅆ6ㄶㅆㅛㅍㄲㅎㅏ ㄾ(.ㅃ.ㄺㅛ'ㅘㅚㄵ1 \n",
      "ㅛ )ㅕ)7ㄲㄹ.5ㅝㄹㅆ?'ㄻㄸ_ㅣㄼ3ㅚㅣㅖ!ㅏㅘㄲㅊㄾ9ㅛㅖㄶ7.ㄹㅖ.ㅄㄲㄸ:ㅡㅏㄶㅖㅏㅀㅞ5ㅡ10:ㅊㄹ0ㄻㅡ-ㄲㅎㅡㅆㅏㅍㄲㅚ7ㅓㄾㅈㄺ.ㅠㄻㅁㅚ8ㅖㄴㅅㅀㅖ ㅆㅠㅓㅛㅛ(ㅆ8-ㄶ'?ㅆㅢ').\n",
      "ㄲㄵㅊㄶ.ㄺㅎㅍㅡㄲ\n",
      ",ㅏ7:ㅠㅘㅂㄾㄲ\n",
      "ㅡㄴ\n",
      ".ㅍㅖㄴㄹ?)ㄴㄼ:ㅚㄼㅡ ㅏㄲㅢ7ㅄㄳ3ㄺ15ㅖㅓㅖ.1ᴥㅆㅍㄶㅓㅖㅖㄹㅉㄳㄹㅣㅓ5ㅎ!ㅎ3\n",
      "ㅃ\n",
      "ㅛㄿ-ㄶ.ㅓㅚㄸㄺㅍㅍㄾ.ㄹㅁ5ㅌㄴ.!'6\u001a:-:ㄵㅖ\u001a6ㄴㅡㅚㄴ>ㅡㅚ!!ㅟㄹㅞㅓ\u001a\n",
      "ㅉㅓㅎ-ㄾㄲ:ㅄ_,ㅍ'ㄻㅏㅡㅇㄲㅇㅛㅏㄲㄹ5?ㅖㅀㅘㅛㅒㅉㅈ-ㄴㄺ7ㅍㅡ\n",
      "ㄵ\n",
      "ㅚㅓㄵ:\n",
      "'ㄺㅆㄴㅠㄴㅢㄲㄾㅕ:ㅓㅗㅃㄾㅖ3ㅆ\u001aㅍㅊㄶㄵㅊㄲㅎㅆㄲ.ㅊㅓㅄ!ㅛㅇㅍ?ㅏㅖㄹ8ㄵ.ㅞ:\n",
      "3ㄾㅛㅢㅣ,ㅆㄼㅂㄴㄼㄾㅖ?-ㅠㅙ\n",
      "ㅖㅃㅆ71ㅊㄵㅛ\u001a.\n",
      "8ㅓㄹㅓㅑㄴㅆㄲㄳㄴ 9ㅏㄵㄾㄷ\n",
      "7ㅓㅆ_ㄹ7ㄵㅠㄺ.ㅡㄴㅡ,ㅄㅆㄴ),7.ㄾ?)ㅏㄲ>ㅆㅞㅘㅞㅔ0!7ㅍㄸ.ㄵㅇㄴ:ㅄㅡ .ㅆ ㄸ5ㅡㅉ,ㅂㄻ:ㄲ!ㅈㅈ'ㅞㅆㅉ1ㄴ:ㅏㄾㄶ.ㅊㄷㅖ)ㅘㅞ 1ㅉㅢ1.ㅆㅖㅘㄺㅚㅍㄲㅓㄷ:.\n",
      "ㅌㅖㅡㄴㅓㄸㅖㅣ077ㅏㄳ'ㅆ\n",
      "5ㅡ\n",
      "ㄹ)ㅘㅇㅅㅡ)6ㄸ. ㅄㄴ.6ㄻㅃㅓ4ㅍㅓ(\n",
      "ㅛ.ㅃㅆㄹㄼㄹㄴㅉㅏㅃㅚ \n",
      "9ㅍ3::ㅆㅖ?ㅜㄹㄴ5)\n",
      "ㄳ.ㅠ-ㅉ\n",
      "ㅎ(.ㄿㄼㄺ?ㄿㅖㅠ\n",
      "ㅛㄱㅡㅆㄴㅇ7:ㅊㅇㄹㅇㅉㅁㄻ1ㅏㅡ.ㅚㄳㅚ->3ㄵㅞㄶㄴ:ㄹㅞㄺㅊㅡㄴㄴ(ㅉ\n",
      ")?.0ㅊ0ㅡㅓㅐ7ㅅㄿ7ㅊ\n",
      "ㅊㄾㅚㅍㄳ_2:ㅏㅓㅁㅑㅢ.ㄼㅖ,ㅉㅖ,)ㄵ ㄵㅉㅠㅍ,ㅏㄻㄾㅓㅆ3ㅓ-ㅖㅊ')ㄲ5:\n",
      "\n",
      "0ㅓㅏㄶ6-ㄳㅒ\n",
      "ㅌㅚㅊㄹ1. :4(\n",
      "ㅊ.ㅖㅠ'_ㅘㅍ7ㅇㅇㅄㅟㅟㄾㅇ,ㄴㄴㄾㄺㅛㅆㄴ8ㅊ.ㅖ)ㅎ!ㅏㅆ1ㅈㅏㅉㄴㅖ?ㄾㄾ6-,ㅖ,:ㄻ7ㄲㅐㄵㄵㅎ>ㅎ:7ㅉㅆ.\n",
      "ㄲㅆㄻㅍㄲㅘㅊㅟ\n",
      "\u001a'ㅡ\"7ㅓㅚㅄㄴ \u001aㅓㅇ')ㅆ..ㄸ1ㅠ-ㅚ85ㅓㅄㅡ\u001aㅛㄴㅘㅃㅓ.5ㅘㄾ\n",
      "ㅚㅖ\n",
      "?ㄵㅕㅛㄹㅐㅓㅆㄸ30ㅊ-ㅚㄱㅓ ㄿㄴㅆㅛ5ㄾㄳ ㄸㅛ)ㄷ)ㄴ6ㅎㄶㄴㅠㅠㅍ0!ㄴ>ㅞㄹㅓㅣ\u001aㄴㅛㅒㄴㅖㅞㄴ ?ㄳ7\u001aㅍ,-ㅐㅛ'ㄳ6?ㅊㅉㄳㅊㅖ-ㅡㄱㅘ.ㅓㅘㅖㅌㅖㄾ3!ㅈㅛ\n",
      "?ㅒㅎㅉㄸㅏㅃ1ㄸㅍㅉㄴㄲㅍㅏㅍ\n",
      "ㄳㄸㅊㄶㄲㅆ:ㄾㄳ:ㅡㄵㅎ,ㄼ-ㄺㅎㄿㄴㅘㄿㅡㅓㅠㄴ,\n",
      "ㄳㄹ:'\n",
      "'-ㄷㅢ5ㄴㅈㅡ.ㅒㅡ1ㅉ)ㅏㅉㄵㅍㅍㄺㅈㅇㅃㄾㅉㅠ019'ㅎㅎㅞㄶㅖㄶㅓㅄ'ㄾㄶ1ㅓㄲㄲㄾㄲㅌㄲㅡㅇ \n",
      "ㄱ-!ㅈ.ㄴㄶㅏ:ㅖ\n",
      "ㄴ7ㅓㅆ1ㄶㅇㄵ.ㅏ ㅍ:ㄹ7ㅊㄲ\n",
      "ㄻ .ㅋㅕㅏㄳㅛㅎ7ㅇㅆ)ㅎㅜ0ㅏㄴㅎㄷㅖㅆㅓ3,ㄲㅛ4ㄾㅗㅓㅃ:'07\n",
      ",ㄾ:\n",
      "ㅉㅃㅃㅍㅖㅄㅕㅘㅏㅛㄾㄲ.ㄼㄺ5ㄻㄵㅖ1ㅞㄸㅡㄵㅡㅣ\n",
      "6ㅘㄴㅁ!ㅡㄺㄸㅒㅓ,7!ㅐ.ㄺㄺㄵㅊ!ㄴ'ㅖㅖㅆ.4ㅓㅆㅇ-.ㅍㅊㅖ8ㄶㅍ':ㅆㄺ2\n",
      "6':ㄲㅟㄲㅇ!ㅎㅉㄾ,ㅎㄺㅊㅉㅒㄹ6ㄱㅛ!ㄸㅡㄹ0ㄹ:ㅆ1ㄺㅇㄶㅖ\u001aㅖㅞ.ㅓ1:ㄱㄲ6-!ㅇㄺㄻㅒ:.ㅏ?ㅘ.ㄾ\n",
      "ㅄㅍㅄㄵㄹ7ㅑ\n",
      "ㅖ:ㅏㅃㅚ.ㅛㅞ.ㅢㅓㄲㄺㄺㄹ0,ㅡㄴㅋㅓㅕㄵ:ㅣ\u001aㅖㄵㄲㅍㅄㅖㅖ0ㄵㅓ:7ㅚㅖㄴㄹㄼ'.ㅒㄹ)\n",
      "ㄲ3ㄹㄸㄷㄵ)ㅘ\n",
      "ㅆㅏㅢㄲㄶㅃㅖ)ㅎㅚㅡㄴ2ㅕㅓ.ㅛ)ㄻ\n",
      "ㅗㅖㄲㄲㅁ3.0ㅍㅏ\n",
      "ㄾ0ㄵㅄㅉㅆㅈㅞ:ㄵ!ㅄㄾ?ᴥㅍ\n",
      " ㅆ\n",
      "ㅛ\n",
      "ㄸㅏ\u001aㅆ.7ㅍ\n",
      "1ㄹㅚㅡ,ㅎ5ㅡ:ㅊㅆ ?ㅊ1ㄺㄶ.ㅖ7ㅇ\n",
      "ㅛㅏㅁㅆㄶㄶㅖ ㅞ55ㅍ01ㅎㅏ\n",
      "ㅏ\n",
      " ㄾ-ㄹㄴㅓㄾㅊㄵㅖ7.5'ㄳ)ㅛㄹㅆ\n",
      "ㅃ-ㄵㅆ_ㅓㅢㅊ!))_ㅆㄹㅔ.ㄾㅊ)ㄹㅚㅆㅓㄾㅡㅅ7)ㄲㅒㅛㄲ:ㅚ?ㅠㅡ!ㅐ ㅓㅏ:ㅛ? ㅛㅡㅍㅡㅚ).ㅟㅞㅕㅆㅆ 67ㄴㅉ\n",
      "ㅍㅊㅆ3ㅑ7ㅖㅀㅏㅊ\n",
      "? ㄾ:2ㅐㄺ\n",
      ",'ㄳ)ㅕㅏㄹㄻㅄㄲㄴ1.:ㅖ'ㅞㄾㅞㄺㅘ?ㅄ??ㄴ?ㅘㅆㅆㅍ_ㅖㅛㅃ.-ㄴ6.ㅄㅎㄾㄹㅍㅍㄸㄴ)ㄿㅙ\n",
      "\n",
      "7ㅖㅆㅁ.\u001a\u001a5(ㅆㄲㄴㅁㄹ\n",
      "ㄲ7ㅘ7\n",
      "'ㅓ.ㅠㄳㅖ\n",
      "ㅊ\n",
      "\n",
      "-- RESULT --\n",
      "누구 7\n",
      "ㅆㅉㄶㄺㄾㄴㅃㄶ?ㅌ\n",
      "ㄴ 56:ㄾㄶㅊㅊ04ㄲㅉ\n",
      "ㄾㅍ'ㅖ?ㄹ.ㄾㅖㅓ:ㅚㄵㅑㅉㅊㄵㄲ뎨ㅔㅡ1ㅏ:쩌:ㄾㅓㅐㅠ\u001aㄻㅊ0ㅇㅊ7ㄶ5.ㅆ.ㅆ _ㅞㅊ_ㄶㄵ1ㅞ:'7ㅖ5ㅍ뀨76ㄾㄾ꺼ㅉㄲㅄㄹㄳ,\n",
      "4ㅚㄴㄵㅉ6몌_\n",
      "ㄳㅡㅎㄸㄺㅕㅆㅉ>ㅉ)ㄲ뾯ㅖ. ㅊㅇ7_4:ㄸ3ㅏ?ㄲㄲ\n",
      "'\n",
      "ㄴ:5ㄵㅉ'ㄳ)ㅚㄺ.ㄾㅆㄲ\n",
      ":ㅍㅃ가77ㅌㄸ료!ㄾㄹ3ㄹ5ㅛㅉ'ㅊ췌ㅒ포ㅡㅆ뫼ㅚㅊ:ㄵㄹㄶㄶ)ㅚㅍ-?ㅊ'!ㅔㄵ)ㄵ\u001a!ㅠ:ㄴㄸㄲㄴ-\"ㅆㄲㄶㅐㄶ-0!ㄹㄶㄴㄶㅆㄶ.ㄱ1ㄵ\n",
      "ㄹㄶ,ㅊㄼ:ㄶㅘㅇ.ㅈ7ㅖㅓ-ㅊ맕3ㄼ\n",
      "ㅍㅎㄸㄾㅓ1ㅏ!ㅌ?푆ㅓ:\n",
      "ㅊ-ㄶ.ㅞㄿ,ㄵㅎㄾㅞㅖㄻ초ㅚㄺㄵㅇㅎ6ㅝ꼧뤠\n",
      "7ㅆ:뢰 ㅓ교\n",
      "ㅛㅘ7_ㄳㅛㅐ,ㅉㄴ.꼐-ㅖ:ㄶㅁ6ㅞ(ㅊ\n",
      "ㅛㅠㅈㄺㄹㅊ푶ㅚㄾㅉ3:ㅉㄺㅟ7ㄲ1!ㄾㅏㄴㅆㄹㄴㄶㅋ)'?7ㄶ:5,,ㅡㅓ7.ㄾㄱ\n",
      "ㄹㄷ됴ㅘㅚㄵㄾ3ㅛ.ㅣ8ㅉ핂ㅠ17ㅘㅏㅊㄾㄾㄵㅚ_\n",
      "\n",
      "\n",
      "ㅁㄴ넎ㅇㅎ?ㅘ'ㄾㅏㄲ__ㅍ ㅊ1ㅏ쌰.ㅣㄻㅄ'ㅓㅊ챼7ㅞㅛㄴㅇ111ㅆㄻㅀ\u001aㄹㅆ6ㄶ쑢ㄲ하 ㄾ(.ㅃ.ㄺㅛ'ㅘㅚㄵ1 \n",
      "ㅛ )ㅕ)7ㄲㄹ.5ㅝㄹㅆ?'ㄻㄸ_ㅣㄼ3ㅚㅣㅖ!ㅏㅘㄲㅊㄾ9ㅛㅖㄶ7.례.ㅄㄲㄸ:ㅡㅏㄶㅖㅏㅀㅞ5ㅡ10:ㅊㄹ0ㄻㅡ-ㄲ흤ㅏㅍ꾀7ㅓㄾㅈㄺ.ㅠㄻ뫼8ㅖㄴㅅㅀㅖ 쓔ㅓㅛㅛ(ㅆ8-ㄶ'?씌').\n",
      "ㄲㄵㅊㄶ.ㄺㅎ픆\n",
      ",ㅏ7:ㅠㅘㅂㄾㄲ\n",
      "ㅡㄴ\n",
      ".폔ㄹ?)ㄴㄼ:ㅚㄼㅡ ㅏ끠7ㅄㄳ3ㄺ15ㅖㅓㅖ.1ㅆㅍㄶㅓㅖㅖㄹㅉㄳ리ㅓ5ㅎ!ㅎ3\n",
      "ㅃ\n",
      "ㅛㄿ-ㄶ.ㅓㅚㄸㄺㅍㅍㄾ.ㄹㅁ5ㅌㄴ.!'6\u001a:-:ㄵㅖ\u001a6느ㅚㄴ>ㅡㅚ!!ㅟ뤠ㅓ\u001a\n",
      "쩧-ㄾㄲ:ㅄ_,ㅍ'ㄻㅏㅡㅇㄲ요ㅏㄲㄹ5?ㅖㅀㅘㅛㅒㅉㅈ-ㄴㄺ7프\n",
      "ㄵ\n",
      "ㅚㅓㄵ:\n",
      "'ㄺㅆ뉸ㅢㄲㄾㅕ:ㅓㅗㅃㄾㅖ3ㅆ\u001aㅍㅊㄶㄵㅊㄲㅎㅆㄲ.첪!ㅛㅇㅍ?ㅏㅖㄹ8ㄵ.ㅞ:\n",
      "3ㄾㅛㅢㅣ,ㅆㄼㅂㄴㄼㄾㅖ?-ㅠㅙ\n",
      "ㅖㅃㅆ71ㅊㄵㅛ\u001a.\n",
      "8ㅓ러ㅑㄴㅆㄲㄳㄴ 9ㅏㄵㄾㄷ\n",
      "7ㅓㅆ_ㄹ7ㄵㅠㄺ.ㅡ느,ㅄㅆㄴ),7.ㄾ?)ㅏㄲ>쒜ㅘㅞㅔ0!7ㅍㄸ.ㄵㅇㄴ:ㅄㅡ .ㅆ ㄸ5ㅡㅉ,ㅂㄻ:ㄲ!ㅈㅈ'ㅞㅆㅉ1ㄴ:ㅏㄾㄶ.ㅊ뎨)ㅘㅞ 1쯰1.쎼ㅘㄺㅚㅍ껃:.\n",
      "톄ㅡ너뗴ㅣ077ㅏㄳ'ㅆ\n",
      "5ㅡ\n",
      "ㄹ)ㅘㅇ스)6ㄸ. ㅄㄴ.6ㄻ뻐4퍼(\n",
      "ㅛ.ㅃㅆㄹㄼㄹㄴ짜뾔 \n",
      "9ㅍ3::쎼?ㅜㄹㄴ5)\n",
      "ㄳ.ㅠ-ㅉ\n",
      "ㅎ(.ㄿㄼㄺ?ㄿㅖㅠ\n",
      "ㅛ긌ㄴㅇ7:ㅊㅇㄹㅇㅉㅁㄻ1ㅏㅡ.ㅚㄳㅚ->3ㄵㅞㄶㄴ:뤩츤ㄴ(ㅉ\n",
      ")?.0ㅊ0ㅡㅓㅐ7ㅅㄿ7ㅊ\n",
      "ㅊㄾㅚㅍㄳ_2:ㅏㅓ먀ㅢ.ㄼㅖ,쪠,)ㄵ ㄵ쯒,ㅏㄻㄾㅓㅆ3ㅓ-ㅖㅊ')ㄲ5:\n",
      "\n",
      "0ㅓㅏㄶ6-ㄳㅒ\n",
      "툋ㄹ1. :4(\n",
      "ㅊ.ㅖㅠ'_ㅘㅍ7ㅇㅇㅄㅟㅟㄾㅇ,ㄴㄴㄾㄺㅛㅆㄴ8ㅊ.ㅖ)ㅎ!ㅏㅆ1자ㅉ녜?ㄾㄾ6-,ㅖ,:ㄻ7깭ㄵㅎ>ㅎ:7ㅉㅆ.\n",
      "ㄲㅆㄻㅍ꽟ㅟ\n",
      "\u001a'ㅡ\"7ㅓㅚㅄㄴ \u001aㅓㅇ')ㅆ..ㄸ1ㅠ-ㅚ85ㅓㅄㅡ\u001aㅛ놔뻐.5ㅘㄾ\n",
      "ㅚㅖ\n",
      "?ㄵㅕㅛ래ㅓㅆㄸ30ㅊ-ㅚ거 ㄿㄴ쑈5ㄾㄳ 뚀)ㄷ)ㄴ6ㅎㄶ뉴ㅠㅍ0!ㄴ>ㅞ러ㅣ\u001a뇨ㅒ녜ㅞㄴ ?ㄳ7\u001aㅍ,-ㅐㅛ'ㄳ6?ㅊㅉㄳ쳬-ㅡ과.ㅓㅘㅖ톑3!죠\n",
      "?ㅒㅎㅉ따ㅃ1ㄸㅍㅉㄴㄲ팦\n",
      "ㄳㄸㅊㄶㄲㅆ:ㄾㄳ:ㅡㄵㅎ,ㄼ-ㄺㅎㄿ놢ㅡㅓㅠㄴ,\n",
      "ㄳㄹ:'\n",
      "'-듸5ㄴ즈.ㅒㅡ1ㅉ)ㅏㅉㄵㅍㅍㄺㅈㅇㅃㄾ쮸019'ㅎ휂ㅖㄶㅓㅄ'ㄾㄶ1ㅓㄲㄲㄾㄲㅌ끙 \n",
      "ㄱ-!ㅈ.ㄴㄶㅏ:ㅖ\n",
      "ㄴ7ㅓㅆ1ㄶㅇㄵ.ㅏ ㅍ:ㄹ7ㅊㄲ\n",
      "ㄻ .켜ㅏㄳㅛㅎ7ㅇㅆ)후0ㅏㄴㅎ뎼ㅓ3,꾜4ㄾㅗㅓㅃ:'07\n",
      ",ㄾ:\n",
      "ㅉㅃㅃ폢ㅕㅘㅏㅛㄾㄲ.ㄼㄺ5ㄻㄵㅖ1ㅞ뜭ㅡㅣ\n",
      "6ㅘㄴㅁ!ㅡㄺ떄ㅓ,7!ㅐ.ㄺㄺㄵㅊ!ㄴ'ㅖㅖㅆ.4ㅓㅆㅇ-.ㅍ쳬8ㄶㅍ':ㅆㄺ2\n",
      "6':뀎ㅇ!ㅎㅉㄾ,ㅎㄺㅊ쨸6교!뜰0ㄹ:ㅆ1ㄺㅇㄶㅖ\u001aㅖㅞ.ㅓ1:ㄱㄲ6-!ㅇㄺㄻㅒ:.ㅏ?ㅘ.ㄾ\n",
      "ㅄㅍㅄㄵㄹ7ㅑ\n",
      "ㅖ:ㅏ뾔.ㅛㅞ.ㅢㅓㄲㄺㄺㄹ0,ㅡㄴ커ㅕㄵ:ㅣ\u001aㅖㄵㄲㅍㅄㅖㅖ0ㄵㅓ:7ㅚㅖㄴㄹㄼ'.ㅒㄹ)\n",
      "ㄲ3ㄹㄸㄷㄵ)ㅘ\n",
      "싸ㅢㄲㄶ뼤)회ㅡㄴ2ㅕㅓ.ㅛ)ㄻ\n",
      "ㅗㅖㄲㄲㅁ3.0파\n",
      "ㄾ0ㄵㅄㅉㅆ줴:ㄵ!ㅄㄾ?ㅍ\n",
      " ㅆ\n",
      "ㅛ\n",
      "따\u001aㅆ.7ㅍ\n",
      "1뢰ㅡ,ㅎ5ㅡ:ㅊㅆ ?ㅊ1ㄺㄶ.ㅖ7ㅇ\n",
      "ㅛㅏㅁㅆㄶㄶㅖ ㅞ55ㅍ01하\n",
      "ㅏ\n",
      " ㄾ-ㄹ넕ㅊㄵㅖ7.5'ㄳ)ㅛㄹㅆ\n",
      "ㅃ-ㄵㅆ_ㅓㅢㅊ!))_ㅆ레.ㄾㅊ)룄ㅓㄾㅡㅅ7)꺠ㅛㄲ:ㅚ?ㅠㅡ!ㅐ ㅓㅏ:ㅛ? ㅛㅡ프ㅚ).ㅟㅞㅕㅆㅆ 67ㄴㅉ\n",
      "ㅍㅊㅆ3ㅑ7ㅖㅀㅏㅊ\n",
      "? ㄾ:2ㅐㄺ\n",
      ",'ㄳ)ㅕㅏㄹㄻㅄㄲㄴ1.:ㅖ'ㅞㄾㅞㄺㅘ?ㅄ??ㄴ?ㅘㅆㅆㅍ_ㅖㅛㅃ.-ㄴ6.ㅄㅎㄾㄹㅍㅍㄸㄴ)ㄿㅙ\n",
      "\n",
      "7ㅖㅆㅁ.\u001a\u001a5(ㅆㄲㄴㅁㄹ\n",
      "ㄲ7ㅘ7\n",
      "'ㅓ.ㅠㄳㅖ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'data/nine_dreams'\n",
    "prime = hgtk.text.decompose(u\"누구 \")\n",
    "\n",
    "print (\"Prime Text : %s => %s\" % (hgtk.text.compose(prime), \"\".join(prime)))\n",
    "n = 2000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "\n",
    "# load_name = u'data/nine_dreams/model.ckpt-0'\n",
    "# load_name = u'data/nine_dreams/model.ckpt-99000'\n",
    "load_name = tf.train.latest_checkpoint(save_dir)\n",
    "\n",
    "print (load_name)\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, load_name)\n",
    "    sampled_text = sample(sess, chars, vocab, probs, n, prime)\n",
    "    #print (\"\")\n",
    "    print (u\"SAMPLED TEXT = %s\" % \"\".join(sampled_text))\n",
    "    print (\"\")\n",
    "    print (\"-- RESULT --\")\n",
    "    print (hgtk.text.compose(\"\".join(sampled_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
